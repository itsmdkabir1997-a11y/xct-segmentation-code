{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"X_iC5cOlefqY"},"outputs":[],"source":["# ============================================================\n","# Cell 1 ‚Äî Setup (paths + dataset staging)\n","#\n","# Run this cell first.\n","#\n","# What this cell does\n","#   1) Selects where training outputs are saved (Drive or VM)\n","#   2) Prompts for the main results folder name (default: Training_Result)\n","#   3) Retrieves the dataset ZIP into the VM (no link spam; progress bar only)\n","#   4) Extracts the ZIP with a progress bar\n","#\n","# Exposed variables\n","#   - RESULTS_ROOT : output root directory\n","#   - DATASET_DIR  : dataset folder inside VM\n","#   - DATA_ROOT    : str(DATASET_DIR) for backward compatibility\n","# ============================================================\n","# ============================================================\n","# Cell 1 ‚Äî Setup (paths + dataset staging)\n","# ============================================================\n","\n","!pip install -q gdown\n","\n","import os\n","import sys\n","\n","# -------------------------\n","# Global experiment config\n","# -------------------------\n","GLOBAL_SEED = 42\n","IN_CHANNELS = 1\n","OUT_CLASSES = 3\n","\n","# --- Determinism guard: this MUST run before any torch import ---\n","if \"torch\" in sys.modules:\n","    raise RuntimeError(\"torch was imported before setting CUBLAS_WORKSPACE_CONFIG. Restart runtime and run CELL 1 first.\")\n","\n","# Optional CPU-side stability (reduces nondeterminism from BLAS/OpenMP scheduling)\n","os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n","os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n","\n","os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n","os.environ[\"PYTHONHASHSEED\"] = str(GLOBAL_SEED)\n","\n","\n","import sys\n","import io\n","from pathlib import Path\n","from zipfile import ZipFile\n","from contextlib import redirect_stdout, redirect_stderr\n","\n","import random\n","import numpy as np\n","\n","import gdown\n","from tqdm import tqdm  # classic tqdm for consistent console bars\n","\n","\n","\n","# -------------------------\n","# CPU-side reproducibility\n","# (Torch-side determinism will be enabled later, after torch import)\n","# -------------------------\n","random.seed(GLOBAL_SEED)\n","np.random.seed(GLOBAL_SEED)\n","\n","print(f\"‚úÖ Cell 1 reproducibility pre-config set (GLOBAL_SEED={GLOBAL_SEED}).\")\n","print(\"   - CUBLAS_WORKSPACE_CONFIG set for deterministic CUDA matmul (effective only if torch not imported yet).\")\n","\n","\n","\n","\n","# -----------------------------\n","# 1) Results destination\n","# -----------------------------\n","print(\n","    \"\\nResults destination\\n\"\n","    \"  1) Google Drive (persistent)\\n\"\n","    \"  2) Colab VM only (temporary)\\n\"\n",")\n","choice = input(\"Select 1 or 2: \").strip()\n","if choice not in {\"1\", \"2\"}:\n","    raise ValueError(\"Invalid choice. Please rerun and select 1 or 2.\")\n","\n","SAVE_TO_DRIVE = (choice == \"1\")\n","\n","if SAVE_TO_DRIVE:\n","    try:\n","        from google.colab import drive\n","        # keep mount output minimal (no extra chatter)\n","        with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):\n","            drive.mount(\"/content/drive\", force_remount=True)\n","    except Exception as e:\n","        raise RuntimeError(\n","            \"Drive mount failed. This cell is intended for Google Colab.\\n\"\n","            f\"Details: {e}\"\n","        )\n","\n","\n","# -----------------------------\n","# 2) Main results folder name\n","# -----------------------------\n","DEFAULT_RESULTS_FOLDER = \"Training_Result\"\n","folder_in = input(f\"Results folder name (default: {DEFAULT_RESULTS_FOLDER}): \").strip()\n","\n","if not folder_in:\n","    RESULTS_FOLDER_NAME = DEFAULT_RESULTS_FOLDER\n","else:\n","    RESULTS_FOLDER_NAME = folder_in.replace(\"\\\\\", \"/\").strip().strip(\"/\")\n","    if not RESULTS_FOLDER_NAME:\n","        RESULTS_FOLDER_NAME = DEFAULT_RESULTS_FOLDER\n","\n","if SAVE_TO_DRIVE:\n","    RESULTS_ROOT = Path(\"/content/drive/MyDrive\") / RESULTS_FOLDER_NAME\n","else:\n","    RESULTS_ROOT = Path(\"/content\") / RESULTS_FOLDER_NAME\n","\n","RESULTS_ROOT.mkdir(parents=True, exist_ok=True)\n","\n","\n","# -----------------------------\n","# 3) Dataset staging in VM\n","# -----------------------------\n","VM_DATA_ROOT = Path(\"/content/data_zip\")\n","VM_DATA_ROOT.mkdir(parents=True, exist_ok=True)\n","\n","ZIP_NAME = \"XCT_SegData_CFRP_FFRP_v1.zip\"\n","LOCAL_ZIP_PATH = VM_DATA_ROOT / ZIP_NAME\n","DATASET_DIR = VM_DATA_ROOT / \"XCT_SegData_CFRP_FFRP_v1\"\n","\n","GDRIVE_ZIP_LINKS = [\n","    \"https://drive.google.com/file/d/195nIFRoL-tEST9WPPYGGMIlpdACsQXWQ/view?usp=sharing\",\n","    \"https://drive.google.com/file/d/1Z1szaBWpD6HYhub3EI9VJAoRPBMGuBQy/view?usp=sharing\",\n","    \"https://drive.google.com/file/d/1EYGAN0yg2V-2rNWSEdk0gm-WH4LkGJcS/view?usp=sharing\",\n","    \"https://drive.google.com/file/d/1IDxrn-XyTgSMj9hRGo5MRmYJxmtAWcxY/view?usp=sharing\",\n","    \"https://drive.google.com/file/d/1inh4XOJdYDQAcT1GigwqlXBpOsOSk78Z/view?usp=sharing\",\n","]\n","\n","DRIVE_ZIP_FALLBACKS = []\n","if SAVE_TO_DRIVE:\n","    DRIVE_ZIP_FALLBACKS = [Path(\"/content/drive/MyDrive\") / ZIP_NAME]\n","\n","\n","# -----------------------------\n","# 4) Helpers (progress bars only)\n","# -----------------------------\n","class _StderrFilter:\n","    \"\"\"\n","    Drops link spam and gdown info lines from stderr while preserving tqdm bars.\n","    \"\"\"\n","    def __init__(self, real_stderr):\n","        self._real = real_stderr\n","\n","    def write(self, s):\n","        if not s:\n","            return 0\n","\n","        # Remove any links completely\n","        if (\"http://\" in s) or (\"https://\" in s):\n","            return 0\n","\n","        # Remove common gdown chatter\n","        if (\"From (original):\" in s) or (\"From (redirected):\" in s) or (\"To:\" in s):\n","            return 0\n","        if s.strip() in {\"Downloading...\", \"Downloading\"}:\n","            return 0\n","\n","        return self._real.write(s)\n","\n","    def flush(self):\n","        return self._real.flush()\n","\n","    def isatty(self):\n","        return getattr(self._real, \"isatty\", lambda: False)()\n","\n","    def fileno(self):\n","        return getattr(self._real, \"fileno\", lambda: 2)()\n","\n","    @property\n","    def encoding(self):\n","        return getattr(self._real, \"encoding\", \"utf-8\")\n","\n","\n","def _copy_file_with_progress(src: Path, dst: Path, chunk_size: int = 1024 * 1024) -> None:\n","    total = src.stat().st_size\n","    with open(src, \"rb\") as fsrc, open(dst, \"wb\") as fdst, tqdm(\n","        total=total,\n","        unit=\"B\",\n","        unit_scale=True,\n","        unit_divisor=1024,\n","        desc=\"Copying\",\n","        dynamic_ncols=True,\n","    ) as pbar:\n","        while True:\n","            buf = fsrc.read(chunk_size)\n","            if not buf:\n","                break\n","            fdst.write(buf)\n","            pbar.update(len(buf))\n","\n","\n","def _download_zip_quiet(urls, out_path: Path) -> bool:\n","    \"\"\"\n","    Keeps the gdown progress bar but removes all link/metadata prints.\n","    \"\"\"\n","    for url in urls:\n","        if out_path.exists():\n","            try:\n","                out_path.unlink()\n","            except Exception:\n","                pass\n","\n","        real_stderr = sys.stderr\n","        sys.stderr = _StderrFilter(real_stderr)\n","        try:\n","            with redirect_stdout(io.StringIO()):\n","                gdown.download(url=url, output=str(out_path), quiet=False, fuzzy=True)\n","        except Exception:\n","            pass\n","        finally:\n","            sys.stderr = real_stderr\n","\n","        if out_path.exists() and out_path.stat().st_size > 0:\n","            return True\n","\n","    return False\n","\n","\n","def _try_drive_fallback(candidates, dst_zip: Path) -> bool:\n","    for c in candidates:\n","        if c.exists() and c.stat().st_size > 0:\n","            _copy_file_with_progress(c, dst_zip)\n","            return True\n","    return False\n","\n","\n","def _unzip_with_progress(zip_path: Path, dest_dir: Path) -> None:\n","    with ZipFile(zip_path, \"r\") as zf:\n","        members = zf.infolist()\n","        total_size = sum(m.file_size for m in members)\n","        with tqdm(\n","            total=total_size,\n","            unit=\"B\",\n","            unit_scale=True,\n","            unit_divisor=1024,\n","            desc=\"Extracting\",\n","            dynamic_ncols=True,\n","        ) as pbar:\n","            for m in members:\n","                zf.extract(m, path=dest_dir)\n","                pbar.update(m.file_size)\n","\n","\n","# -----------------------------\n","# 5) Ensure dataset is ready\n","# -----------------------------\n","if not (DATASET_DIR.exists() and any(DATASET_DIR.iterdir())):\n","    if not (LOCAL_ZIP_PATH.exists() and LOCAL_ZIP_PATH.stat().st_size > 0):\n","        ok = _download_zip_quiet(GDRIVE_ZIP_LINKS, LOCAL_ZIP_PATH)\n","\n","        if not ok and SAVE_TO_DRIVE and DRIVE_ZIP_FALLBACKS:\n","            ok = _try_drive_fallback(DRIVE_ZIP_FALLBACKS, LOCAL_ZIP_PATH)\n","\n","        if not ok:\n","            raise RuntimeError(\"Dataset ZIP could not be retrieved (all sources failed).\")\n","\n","    _unzip_with_progress(LOCAL_ZIP_PATH, VM_DATA_ROOT)\n","\n","    if not (DATASET_DIR.exists() and any(DATASET_DIR.iterdir())):\n","        raise RuntimeError(\"Extraction completed but dataset folder is missing/empty.\")\n","\n","DATA_ROOT = str(DATASET_DIR)\n","\n","\n","\n","# ============================================================\n","# Dataset presence + compact dataset tree (no extra output)\n","# ============================================================\n","\n","from pathlib import Path\n","\n","def _count_files(p: Path) -> int:\n","    try:\n","        return sum(1 for x in p.iterdir() if x.is_file())\n","    except Exception:\n","        return 0\n","\n","def print_dataset_tree(dataset_dir: Path):\n","    dataset_dir = Path(dataset_dir)\n","    print(\"üìÇ Checking dataset folder presence...\")\n","    print(\"Exists?\", dataset_dir.exists())\n","\n","    print(\"DATASET TREE\")\n","    print(f\"{dataset_dir.name}/\")\n","\n","    domains = [\n","        (\"CFRP\", \"‚îú‚îÄ‚îÄ\"),\n","        (\"FFRP_Autoclave\", \"‚îú‚îÄ‚îÄ\"),\n","        (\"FFRP_Oven\", \"‚îî‚îÄ‚îÄ\"),\n","    ]\n","\n","    for domain, prefix in domains:\n","        d = dataset_dir / domain\n","        if not d.exists():\n","            continue\n","\n","        msk = d / \"MASK\"\n","        xct = d / \"XCT\"\n","        msk_n = _count_files(msk)\n","        xct_n = _count_files(xct)\n","\n","        print(f\"{prefix} {domain}/\")\n","        print(\"‚îÇ   ‚îú‚îÄ‚îÄ MASK/  (files: {})\".format(msk_n))\n","        print(\"‚îÇ   ‚îî‚îÄ‚îÄ XCT/   (files: {})\".format(xct_n))\n","\n","print_dataset_tree(Path(DATASET_DIR))\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-d2_A3Yf4ng"},"outputs":[],"source":["# ============================================================\n","# CELL 2: Experiment selection + split + datasets/dataloaders + RUN_META\n","# ============================================================\n","\n","import os\n","import random\n","from contextlib import contextmanager\n","from pathlib import Path\n","from typing import Dict, List, Tuple\n","\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","import tifffile as tiff\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import cv2  # for PadIfNeeded border_mode\n","\n","\n","# -----------------------------\n","# STRICT determinism + TF32 OFF (paper-grade reproducibility)\n","# -----------------------------\n","DETERMINISM_DEBUG = True  # keep True while debugging reproducibility\n","\n","# Disable TF32 / enforce IEEE FP32 (new API; avoids deprecation warnings)\n","try:\n","    torch.backends.cuda.matmul.fp32_precision = \"ieee\"\n","except Exception:\n","    pass\n","try:\n","    torch.backends.cudnn.conv.fp32_precision = \"ieee\"\n","except Exception:\n","    pass\n","\n","# cuDNN determinism\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","\n","if DETERMINISM_DEBUG:\n","    # Do not swallow errors: fail fast if any nondeterministic op is used.\n","    torch.use_deterministic_algorithms(True)\n","    try:\n","        torch.set_deterministic_debug_mode(\"error\")\n","    except Exception:\n","        pass\n","\n","# OpenCV determinism: disable threading + OpenCL\n","try:\n","    cv2.setNumThreads(0)\n","    cv2.ocl.setUseOpenCL(False)\n","except Exception:\n","    pass\n","\n","# Reduce CPU scheduling variability (mostly irrelevant when NUM_WORKERS=0, but safe)\n","try:\n","    torch.set_num_threads(1)\n","    torch.set_num_interop_threads(1)\n","except Exception:\n","    pass\n","\n","\n","# -----------------------------\n","# Deterministic seed helpers\n","# -----------------------------\n","def reset_all_seeds(seed: int = 42):\n","    \"\"\"Reset Python/NumPy/PyTorch RNG state for reproducible runs.\"\"\"\n","    seed = int(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","\n","    random.seed(seed)\n","    np.random.seed(seed)\n","\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","\n","    # CV2 RNG (albumentations uses cv2 internally sometimes)\n","    try:\n","        cv2.setRNGSeed(seed)\n","    except Exception:\n","        pass\n","\n","    # cuDNN determinism (repeat for safety)\n","    try:\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","    except Exception:\n","        pass\n","\n","\n","@contextmanager\n","def fixed_rng(seed: int):\n","    \"\"\"\n","    Freeze RNG state for deterministic per-sample augmentation.\n","    IMPORTANT: do NOT touch CUDA RNG here (dataset __getitem__ must be CPU-only RNG).\n","    \"\"\"\n","    seed = int(seed)\n","\n","    py_state = random.getstate()\n","    np_state = np.random.get_state()\n","\n","    random.seed(seed)\n","    np.random.seed(seed)\n","\n","    # OpenCV RNG for deterministic geometric ops\n","    try:\n","        cv2.setRNGSeed(seed)\n","    except Exception:\n","        pass\n","\n","    try:\n","        yield\n","    finally:\n","        random.setstate(py_state)\n","        np.random.set_state(np_state)\n","\n","        # restore cv2 to base seed (optional but keeps global stable)\n","        try:\n","            cv2.setRNGSeed(int(GLOBAL_SEED))\n","        except Exception:\n","            pass\n","\n","\n","# -----------------------------\n","# Global config (safe defaults)\n","# -----------------------------\n","if \"DATASET_DIR\" not in globals():\n","    DATASET_DIR = Path(\"/content/data_zip/XCT_SegData_CFRP_FFRP_v1\")\n","else:\n","    DATASET_DIR = Path(DATASET_DIR)\n","\n","if \"GLOBAL_SEED\" not in globals():\n","    GLOBAL_SEED = 42\n","\n","if \"IN_CHANNELS\" not in globals():\n","    IN_CHANNELS = 1\n","if \"OUT_CLASSES\" not in globals():\n","    OUT_CLASSES = 3\n","\n","# Loader config (keep here; ablation toggles will be in a separate config cell)\n","TRAIN_FRAC  = 0.8\n","PATCH_SIZE  = 256\n","BATCH_SIZE  = 4\n","NUM_WORKERS = 0  # you said this will ALWAYS be 0\n","\n","reset_all_seeds(int(GLOBAL_SEED))\n","\n","\n","# -----------------------------\n","# Domains and file pairing\n","# -----------------------------\n","DOMAIN_INFO = {\n","    \"CFRP\": {\n","        \"xct_dir\":  DATASET_DIR / \"CFRP\" / \"XCT\",\n","        \"mask_dir\": DATASET_DIR / \"CFRP\" / \"MASK\",\n","    },\n","    \"FFRP_Autoclave\": {\n","        \"xct_dir\":  DATASET_DIR / \"FFRP_Autoclave\" / \"XCT\",\n","        \"mask_dir\": DATASET_DIR / \"FFRP_Autoclave\" / \"MASK\",\n","    },\n","    \"FFRP_Oven\": {\n","        \"xct_dir\":  DATASET_DIR / \"FFRP_Oven\" / \"XCT\",\n","        \"mask_dir\": DATASET_DIR / \"FFRP_Oven\" / \"MASK\",\n","    },\n","}\n","\n","def collect_pairs(xct_dir: Path, mask_dir: Path) -> List[Tuple[Path, Path]]:\n","    \"\"\"\n","    Match XCT/MASK pairs by filename stem. Supports .tif/.tiff.\n","    Returns sorted list of (xct_path, mask_path).\n","    \"\"\"\n","    xct_dir  = Path(xct_dir)\n","    mask_dir = Path(mask_dir)\n","\n","    x_files = sorted([p for p in xct_dir.iterdir() if p.is_file() and p.suffix.lower() in {\".tif\", \".tiff\"}])\n","    m_files = sorted([p for p in mask_dir.iterdir() if p.is_file() and p.suffix.lower() in {\".tif\", \".tiff\"}])\n","\n","    m_map = {p.stem: p for p in m_files}\n","\n","    pairs = []\n","    for xp in x_files:\n","        mp = m_map.get(xp.stem, None)\n","        if mp is not None:\n","            pairs.append((xp, mp))\n","    return pairs\n","\n","ALL_PAIRS_BY_DOMAIN: Dict[str, List[Tuple[Path, Path]]] = {}\n","for dom, info in DOMAIN_INFO.items():\n","    if not info[\"xct_dir\"].exists() or not info[\"mask_dir\"].exists():\n","        raise FileNotFoundError(f\"Missing domain folders for: {dom}\")\n","    ALL_PAIRS_BY_DOMAIN[dom] = collect_pairs(info[\"xct_dir\"], info[\"mask_dir\"])\n","\n","\n","# -----------------------------\n","# Experiment definitions (Core A4 placed right after A3)\n","# -----------------------------\n","EXPERIMENTS = {\n","    \"1\":  {\"name\": \"A1_train_CFRP__val_CFRP\",               \"short\": \"A1\", \"train_domains\": [\"CFRP\"],             \"val_domains\": [\"CFRP\"]},\n","    \"2\":  {\"name\": \"A2_train_Autoclave__val_Autoclave\",     \"short\": \"A2\", \"train_domains\": [\"FFRP_Autoclave\"],   \"val_domains\": [\"FFRP_Autoclave\"]},\n","    \"3\":  {\"name\": \"A3_train_Oven__val_Oven\",               \"short\": \"A3\", \"train_domains\": [\"FFRP_Oven\"],        \"val_domains\": [\"FFRP_Oven\"]},\n","\n","    # Core experiment (A4): All domains ‚Üí All domains\n","    \"4\":  {\"name\": \"Core_train_AllDomains__val_AllDomains\", \"short\": \"A4\",\n","           \"train_domains\": [\"CFRP\", \"FFRP_Autoclave\", \"FFRP_Oven\"],\n","           \"val_domains\":   [\"CFRP\", \"FFRP_Autoclave\", \"FFRP_Oven\"]},\n","\n","    \"5\":  {\"name\": \"B1_train_CFRP__val_AllDomains\",         \"short\": \"B1\", \"train_domains\": [\"CFRP\"],           \"val_domains\": [\"CFRP\", \"FFRP_Autoclave\", \"FFRP_Oven\"]},\n","    \"6\":  {\"name\": \"B2_train_Autoclave__val_AllDomains\",    \"short\": \"B2\", \"train_domains\": [\"FFRP_Autoclave\"], \"val_domains\": [\"CFRP\", \"FFRP_Autoclave\", \"FFRP_Oven\"]},\n","    \"7\":  {\"name\": \"B3_train_Oven__val_AllDomains\",         \"short\": \"B3\", \"train_domains\": [\"FFRP_Oven\"],      \"val_domains\": [\"CFRP\", \"FFRP_Autoclave\", \"FFRP_Oven\"]},\n","\n","    \"8\":  {\"name\": \"C1_train_CFRP+Oven__val_Autoclave\",     \"short\": \"C1\", \"train_domains\": [\"CFRP\", \"FFRP_Oven\"],              \"val_domains\": [\"FFRP_Autoclave\"]},\n","    \"9\":  {\"name\": \"C2_train_CFRP+Autoclave__val_Oven\",     \"short\": \"C2\", \"train_domains\": [\"CFRP\", \"FFRP_Autoclave\"],         \"val_domains\": [\"FFRP_Oven\"]},\n","    \"10\": {\"name\": \"C3_train_Autoclave+Oven__val_CFRP\",     \"short\": \"C3\", \"train_domains\": [\"FFRP_Autoclave\", \"FFRP_Oven\"],     \"val_domains\": [\"CFRP\"]},\n","}\n","\n","def show_experiment_menu():\n","    print(\"\\n================ EXPERIMENT MENU ================\")\n","    print(\"Group A: Single-domain baselines (in-domain)\")\n","    print(\"  1) A1 : CFRP ‚Üí CFRP\")\n","    print(\"  2) A2 : FFRP_Autoclave ‚Üí FFRP_Autoclave\")\n","    print(\"  3) A3 : FFRP_Oven ‚Üí FFRP_Oven\")\n","    print(\"\\nCore experiment (A4): All domains ‚Üí All domains\")\n","    print(\"  4) A4 : {CFRP, FFRP_Autoclave, FFRP_Oven} ‚Üí {CFRP, FFRP_Autoclave, FFRP_Oven}\\n\")\n","\n","    print(\"Group B: Single-domain train, eval on ALL domains\")\n","    print(\"  5) B1 : CFRP ‚Üí {CFRP, FFRP_Autoclave, FFRP_Oven}\")\n","    print(\"  6) B2 : FFRP_Autoclave ‚Üí {CFRP, FFRP_Autoclave, FFRP_Oven}\")\n","    print(\"  7) B3 : FFRP_Oven ‚Üí {CFRP, FFRP_Autoclave, FFRP_Oven}\\n\")\n","\n","    print(\"Group C: Multi-domain training (cross-domain)\")\n","    print(\"  8)  C1 : {CFRP, FFRP_Oven} ‚Üí FFRP_Autoclave\")\n","    print(\"  9)  C2 : {CFRP, FFRP_Autoclave} ‚Üí FFRP_Oven\")\n","    print(\" 10)  C3 : {FFRP_Autoclave, FFRP_Oven} ‚Üí CFRP\")\n","    print(\"=================================================\\n\")\n","\n","show_experiment_menu()\n","choice = input(\"Choose experiment (1‚Äì10): \").strip()\n","if choice not in EXPERIMENTS:\n","    raise ValueError(f\"Invalid experiment choice: {choice}\")\n","\n","exp_cfg = EXPERIMENTS[choice]\n","EXPERIMENT_NAME  = exp_cfg[\"name\"]\n","EXPERIMENT_SHORT = exp_cfg[\"short\"]\n","TRAIN_DOMAINS    = list(exp_cfg[\"train_domains\"])\n","VAL_DOMAINS      = list(exp_cfg[\"val_domains\"])\n","\n","# Run name convention stays unchanged\n","RUN_NAME = f\"{EXPERIMENT_SHORT}_seed{int(GLOBAL_SEED)}\"\n","\n","print(\"\\nSelected setup\")\n","print(f\"  Experiment : {EXPERIMENT_NAME}\")\n","print(f\"  Short      : {EXPERIMENT_SHORT}\")\n","print(f\"  Run name   : {RUN_NAME}\")\n","print(f\"  Train doms : {TRAIN_DOMAINS}\")\n","print(f\"  Val doms   : {VAL_DOMAINS}\")\n","\n","\n","# -----------------------------\n","# Per-experiment split\n","# -----------------------------\n","def per_experiment_split(all_pairs_by_domain, train_domains, val_domains, seed: int, exp_choice: int):\n","    \"\"\"\n","    Per-experiment splitting:\n","      - Train-only domain ‚Üí all slices in train\n","      - Val-only domain   ‚Üí ~20% slices in val\n","      - Train+Val domain  ‚Üí ~80% train / ~20% val\n","    \"\"\"\n","    train_pairs_by_domain = {}\n","    val_pairs_by_domain   = {}\n","\n","    all_used  = sorted(set(train_domains + val_domains))\n","    dom_order = list(sorted(DOMAIN_INFO.keys()))  # stable\n","\n","    for dom in all_used:\n","        pairs = all_pairs_by_domain[dom]\n","        n_total = len(pairs)\n","\n","        in_train = dom in train_domains\n","        in_val   = dom in val_domains\n","\n","        train_idx, val_idx = [], []\n","\n","        if in_train and in_val:\n","            indices = list(range(n_total))\n","            dom_idx = dom_order.index(dom)\n","\n","            rng_dom = random.Random(int(seed) + 1000 * int(exp_choice) + 100 * (dom_idx + 1))\n","            rng_dom.shuffle(indices)\n","\n","            n_train = int(round(n_total * TRAIN_FRAC))\n","            train_idx = indices[:n_train]\n","            val_idx   = indices[n_train:]\n","\n","        elif in_train and not in_val:\n","            train_idx = list(range(n_total))\n","\n","        elif in_val and not in_train:\n","            indices = list(range(n_total))\n","            dom_idx = dom_order.index(dom)\n","\n","            rng_dom = random.Random(int(seed) + 1000 * int(exp_choice) + 100 * (dom_idx + 1))\n","            rng_dom.shuffle(indices)\n","\n","            n_val = max(1, int(round(n_total * (1.0 - TRAIN_FRAC))))\n","            val_idx = indices[:n_val]\n","\n","        if len(train_idx) > 0:\n","            train_pairs_by_domain[dom] = [pairs[j] for j in train_idx]\n","        if len(val_idx) > 0:\n","            val_pairs_by_domain[dom] = [pairs[j] for j in val_idx]\n","\n","    return train_pairs_by_domain, val_pairs_by_domain\n","\n","\n","def balance_train_pairs(pairs_by_domain, seed: int):\n","    \"\"\"Downsample all train domains to the minimum slice count among active train domains.\"\"\"\n","    lengths = {d: len(v) for d, v in pairs_by_domain.items() if len(v) > 0}\n","    if not lengths:\n","        return pairs_by_domain\n","\n","    min_count = min(lengths.values())\n","    rng = random.Random(int(seed) + 999)\n","\n","    balanced = {}\n","    for d, pairs in pairs_by_domain.items():\n","        balanced[d] = rng.sample(pairs, min_count) if len(pairs) > min_count else list(pairs)\n","    return balanced\n","\n","\n","def balance_val_pairs(pairs_by_domain, seed: int, val_domains):\n","    \"\"\"Downsample validation across active val domains to the minimum slice count.\"\"\"\n","    lengths = {d: len(pairs_by_domain.get(d, [])) for d in val_domains if len(pairs_by_domain.get(d, [])) > 0}\n","    if len(lengths) <= 1:\n","        return pairs_by_domain\n","\n","    min_count = min(lengths.values())\n","    rng = random.Random(int(seed) + 1234)\n","\n","    balanced = {}\n","    for d in val_domains:\n","        pairs = pairs_by_domain.get(d, [])\n","        balanced[d] = rng.sample(pairs, min_count) if len(pairs) > min_count else list(pairs)\n","    return balanced\n","\n","\n","train_pairs_raw, val_pairs_raw = per_experiment_split(\n","    ALL_PAIRS_BY_DOMAIN, TRAIN_DOMAINS, VAL_DOMAINS,\n","    seed=int(GLOBAL_SEED),\n","    exp_choice=int(choice)\n",")\n","\n","train_pairs_by_domain = balance_train_pairs(train_pairs_raw, int(GLOBAL_SEED))\n","val_pairs_by_domain   = balance_val_pairs(val_pairs_raw, int(GLOBAL_SEED), VAL_DOMAINS)\n","\n","# Flatten train items\n","train_items = []\n","for d, pairs in train_pairs_by_domain.items():\n","    for x_path, m_path in pairs:\n","        train_items.append({\"domain\": d, \"xct_path\": x_path, \"mask_path\": m_path})\n","\n","# Val items (kept domain-wise)\n","val_items_by_domain = {}\n","for d in VAL_DOMAINS:\n","    pairs = val_pairs_by_domain.get(d, [])\n","    val_items_by_domain[d] = [{\"domain\": d, \"xct_path\": x_path, \"mask_path\": m_path} for x_path, m_path in pairs]\n","\n","def _count_domain(items, dom):\n","    return sum(1 for it in items if it[\"domain\"] == dom)\n","\n","val_total = sum(len(v) for v in val_items_by_domain.values())\n","\n","print(\"\\nFinal split (after balancing)\")\n","print(f\"  Train total: {len(train_items)}\")\n","for d in TRAIN_DOMAINS:\n","    print(f\"    - {d}: {_count_domain(train_items, d)}\")\n","print(f\"  Val total  : {val_total}\")\n","for d in VAL_DOMAINS:\n","    print(f\"    - {d}: {len(val_items_by_domain.get(d, []))}\")\n","\n","\n","# -----------------------------\n","# Transforms (256√ó256 crop; no resize)\n","# -----------------------------\n","def get_transforms(is_train: bool):\n","    \"\"\"\n","    No resize. Always:\n","      - PadIfNeeded (reflect) to ensure >= PATCH_SIZE\n","      - Train: RandomCrop + light augmentation\n","      - Val: CenterCrop\n","    \"\"\"\n","    aug_list = [\n","        A.PadIfNeeded(\n","            min_height=PATCH_SIZE,\n","            min_width=PATCH_SIZE,\n","            border_mode=cv2.BORDER_REFLECT_101,\n","        )\n","    ]\n","\n","    if is_train:\n","        aug_list += [\n","            A.HorizontalFlip(p=0.5),\n","            A.VerticalFlip(p=0.5),\n","            A.RandomBrightnessContrast(brightness_limit=0.20, contrast_limit=0.20, p=0.3),\n","            A.RandomCrop(height=PATCH_SIZE, width=PATCH_SIZE),\n","        ]\n","    else:\n","        aug_list += [A.CenterCrop(height=PATCH_SIZE, width=PATCH_SIZE)]\n","\n","    aug_list.append(ToTensorV2())\n","    return A.Compose(aug_list)\n","\n","TRAIN_TRANSFORM = get_transforms(is_train=True)\n","VAL_TRANSFORM   = get_transforms(is_train=False)\n","\n","\n","# -----------------------------\n","# Dataset (per-image min-max; epoch-aware deterministic aug per-sample)\n","# -----------------------------\n","def per_image_minmax(img: np.ndarray) -> np.ndarray:\n","    img = img.astype(np.float32)\n","    vmin = float(img.min())\n","    vmax = float(img.max())\n","    if vmax > vmin:\n","        return (img - vmin) / (vmax - vmin)\n","    return np.zeros_like(img, dtype=np.float32)\n","\n","\n","class SegmentationDataset(Dataset):\n","    \"\"\"\n","    Returns: (image_tensor, mask_tensor, domain_str)\n","\n","    Epoch-aware deterministic schedule:\n","      seed = base_seed + epoch*BIG + idx\n","    So:\n","      - same epoch => all models get same augmentation for same idx\n","      - different epoch => augmentation changes (crop/flip/etc)\n","    \"\"\"\n","    def __init__(self, items, transform=None, base_seed: int = 0):\n","        self.items = list(items)\n","        self.transform = transform\n","        self.base_seed = int(base_seed)\n","        self.epoch = 0\n","\n","    def set_epoch(self, epoch: int):\n","        self.epoch = int(epoch)\n","\n","    def __len__(self):\n","        return len(self.items)\n","\n","    def __getitem__(self, idx):\n","        info = self.items[int(idx)]\n","        x_path = info[\"xct_path\"]\n","        m_path = info[\"mask_path\"]\n","        domain = info[\"domain\"]\n","\n","        img  = tiff.imread(str(x_path)).astype(np.float32)\n","        mask = tiff.imread(str(m_path)).astype(np.uint8)\n","\n","        img = per_image_minmax(img)\n","\n","        # Albumentations expects HWC\n","        img_ch = img[..., None] if img.ndim == 2 else img\n","\n","        if self.transform is not None:\n","            BIG = 1_000_000\n","            seed = self.base_seed + (int(self.epoch) * BIG) + int(idx)\n","\n","            if hasattr(self.transform, \"set_random_seed\"):\n","                try:\n","                    self.transform.set_random_seed(int(seed))\n","                except Exception:\n","                    pass\n","\n","            with fixed_rng(seed):\n","                aug = self.transform(image=img_ch, mask=mask)\n","\n","            img_t  = aug[\"image\"].float()\n","            mask_t = aug[\"mask\"].long()\n","        else:\n","            img_t  = torch.from_numpy(img_ch.transpose(2, 0, 1)).float()\n","            mask_t = torch.from_numpy(mask).long()\n","\n","        return img_t, mask_t, domain\n","\n","\n","def worker_init_fn(worker_id):\n","    # NUM_WORKERS is always 0 in your setup; keep minimal.\n","    worker_seed = int(GLOBAL_SEED) + int(worker_id)\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","\n","\n","# -----------------------------\n","# DataLoader factory (rebuild per model for identical schedule)\n","# -----------------------------\n","def make_dataloaders(base_seed: int = None):\n","    \"\"\"\n","    Build new loaders with a deterministic generator.\n","    Call this at the start of EACH model training to keep the same shuffle schedule.\n","    \"\"\"\n","    global TRAIN_SHUFFLE_BASE_SEED\n","    if base_seed is None:\n","        base_seed = int(GLOBAL_SEED)\n","    TRAIN_SHUFFLE_BASE_SEED = int(base_seed)\n","\n","    tr_ds = SegmentationDataset(train_items, transform=TRAIN_TRANSFORM, base_seed=int(base_seed))\n","\n","    val_flat = []\n","    for _, dom_list in val_items_by_domain.items():\n","        val_flat.extend(dom_list)\n","    va_ds = SegmentationDataset(val_flat, transform=VAL_TRANSFORM, base_seed=int(base_seed) + 12345)\n","\n","    tr_gen = torch.Generator()\n","    tr_gen.manual_seed(int(base_seed))\n","\n","    tr_loader = DataLoader(\n","        tr_ds,\n","        batch_size=BATCH_SIZE,\n","        shuffle=True,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=True,\n","        worker_init_fn=worker_init_fn,\n","        generator=tr_gen,\n","    )\n","\n","    # IMPORTANT: attach the real generator so epoch reseeding is guaranteed across PyTorch versions\n","    tr_loader._shuffle_gen = tr_gen\n","\n","    va_loader = DataLoader(\n","        va_ds,\n","        batch_size=BATCH_SIZE,\n","        shuffle=False,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=True,\n","        worker_init_fn=worker_init_fn,\n","    )\n","\n","    # Per-domain eval loaders (center crop, no shuffle)\n","    tr_eval_by_dom = {}\n","    for d in TRAIN_DOMAINS:\n","        dom_items = [it for it in train_items if it[\"domain\"] == d]\n","        if len(dom_items) == 0:\n","            continue\n","        ds_d = SegmentationDataset(dom_items, transform=VAL_TRANSFORM, base_seed=int(base_seed) + 777)\n","        tr_eval_by_dom[d] = DataLoader(ds_d, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n","\n","    val_by_dom = {}\n","    for d in VAL_DOMAINS:\n","        dom_items = val_items_by_domain.get(d, [])\n","        if len(dom_items) == 0:\n","            continue\n","        ds_d = SegmentationDataset(dom_items, transform=VAL_TRANSFORM, base_seed=int(base_seed) + 888)\n","        val_by_dom[d] = DataLoader(ds_d, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n","\n","    return tr_ds, va_ds, tr_loader, va_loader, tr_eval_by_dom, val_by_dom\n","\n","# leave make_dataloaders(...) defined, but DO NOT call it here\n","print(\"\\n‚úÖ Cell 2 ready: splits prepared (train_items / val_items_by_domain).\")\n","print(\"   NOTE: dataloaders will be built in the training cell for each model.\")\n","\n","\n","def set_training_epoch(epoch: int):\n","    \"\"\"\n","    Called by training loop at the start of each epoch.\n","\n","    Goal (your requirement):\n","      - SAME epoch: all models see SAME shuffle order + SAME augmentation\n","      - DIFF epoch: augmentation changes deterministically; shuffle stays the same deterministic order\n","\n","\n","    Mechanism:\n","      - train_dataset.set_epoch(epoch) controls epoch-wise augmentation\n","      - reseed the DataLoader generator with base_seed (fixed) for deterministic fixed shuffle across all epochs\n","\n","    \"\"\"\n","    train_dataset.set_epoch(int(epoch))\n","\n","    gen = getattr(train_loader, \"_shuffle_gen\", None)\n","    if isinstance(gen, torch.Generator):\n","        gen.manual_seed(int(TRAIN_SHUFFLE_BASE_SEED))\n","\n","\n","# -----------------------------\n","# RUN_META (stored once in Table A later)\n","# -----------------------------\n","RUN_META = {\n","    \"experiment\": EXPERIMENT_NAME,\n","    \"experiment_short\": EXPERIMENT_SHORT,\n","    \"run_name\": RUN_NAME,\n","    \"seed\": int(GLOBAL_SEED),\n","    \"train_domains\": \",\".join(TRAIN_DOMAINS),\n","    \"val_domains\": \",\".join(VAL_DOMAINS),\n","    \"train_total\": len(train_items),\n","    \"val_total\": int(val_total),\n","}\n","for d in [\"CFRP\", \"FFRP_Autoclave\", \"FFRP_Oven\"]:\n","    RUN_META[f\"n_train_{d}\"] = _count_domain(train_items, d)\n","    RUN_META[f\"n_val_{d}\"]   = len(val_items_by_domain.get(d, []))\n","\n","\n","# -----------------------------\n","# Console table helpers (training loop will use these)\n","# -----------------------------\n","def print_epoch_table_header():\n","    print(\"\")\n","    print(\"‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n","    print(\"‚îÇ Epoch ‚îÇ Phase ‚îÇ    LR    ‚îÇ LossTot  ‚îÇ LossCls  ‚îÇ LossDice ‚îÇ HardDice  ‚îÇ  HardIoU  ‚îÇ Time(s)‚îÇ\")\n","    print(\"‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\n","\n","def format_epoch_row(epoch: int, phase: str, lr: float,\n","                     loss_total: float, loss_cls: float, loss_dice: float,\n","                     hard_dice: float, hard_iou: float, time_sec: float) -> str:\n","    phase = (phase or \"\")[:5]\n","    return (\n","        \"‚îÇ {e:>5d} ‚îÇ {p:<5s} ‚îÇ {lr:>8.2e} ‚îÇ {lt:>8.5f} ‚îÇ {lc:>8.5f} ‚îÇ {ld:>8.5f} ‚îÇ {dc:>9.5f} ‚îÇ {io:>9.5f} ‚îÇ {ts:>6.2f} ‚îÇ\"\n","        .format(e=epoch, p=phase, lr=lr, lt=loss_total, lc=loss_cls, ld=loss_dice, dc=hard_dice, io=hard_iou, ts=time_sec)\n","    )\n","\n","def print_epoch_table_footer():\n","    print(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n","\n","\n","print(\"\\n‚úÖ Cell 2 ready: splits/transforms defined (train_items / val_items_by_domain / RUN_META).\")\n","print(\"   NOTE: dataloaders will be built in the training cell for each model.\")\n","print(f\"   RUN_NAME = {RUN_NAME}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nXYSwy_hpOGJ"},"outputs":[],"source":["# ============================================================\n","# Visualisation (train + val) ‚Äî per-domain one row:\n","#   col1: raw XCT (full image, min‚Äìmax)\n","#   col2: raw MASK (full image)  [fixed colors]\n","#   col3: augmented XCT (PATCH_SIZE√óPATCH_SIZE)\n","#   col4: augmented MASK (PATCH_SIZE√óPATCH_SIZE) [fixed colors]\n","#\n","# OPTION B INCLUDED:\n","#   If train_loader is not defined yet (by design in Cell 2),\n","#   build a LOCAL viz_loader for a quick batch sanity check,\n","#   without touching training globals.\n","#\n","# Determinism safety:\n","# - NO global seeding here (no random.seed / np.random.seed / torch.manual_seed).\n","# - Local RNG objects only.\n","# - If fixed_rng(seed) exists, we use it only as a context manager that restores RNG.\n","# ============================================================\n","\n","import random\n","import numpy as np\n","import tifffile as tiff\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap, BoundaryNorm\n","\n","import torch\n","from torch.utils.data import DataLoader\n","\n","# -----------------------------\n","# Fixed mask colors (label -> color)\n","# -----------------------------\n","MASK_COLOR_ORDER = [\"green\", \"blue\", \"red\"]  # index 0,1,2\n","MASK_CMAP = ListedColormap(MASK_COLOR_ORDER)\n","MASK_NORM = BoundaryNorm([-0.5, 0.5, 1.5, 2.5], MASK_CMAP.N)\n","\n","# -----------------------------\n","# Local unpacker (NO dependency on Cell 4.5B)\n","# -----------------------------\n","def viz_unpack_batch(b):\n","    \"\"\"\n","    Supports:\n","      - tuple/list: (x, y) or (x, y, domain)\n","      - dict: {\"image\":..., \"mask\":..., \"domain\":...}\n","    \"\"\"\n","    if isinstance(b, dict):\n","        return b.get(\"image\"), b.get(\"mask\"), b.get(\"domain\")\n","    if isinstance(b, (list, tuple)):\n","        if len(b) == 2:\n","            return b[0], b[1], None\n","        return b[0], b[1], b[2]\n","    raise ValueError(\"Unsupported batch format\")\n","\n","# -----------------------------\n","# Helpers: Tensor/NumPy -> 2D arrays for imshow\n","# -----------------------------\n","def _as_np_img(x):\n","    \"\"\"Return a 2D numpy array for grayscale display.\"\"\"\n","    if torch.is_tensor(x):\n","        arr = x.detach().cpu().numpy()\n","        if arr.ndim == 3 and arr.shape[0] == 1:      # (1,H,W)\n","            arr = arr[0]\n","        elif arr.ndim == 3 and arr.shape[-1] == 1:   # (H,W,1)\n","            arr = arr[..., 0]\n","        return arr.astype(np.float32)\n","\n","    arr = np.asarray(x)\n","    if arr.ndim == 3 and arr.shape[-1] == 1:\n","        arr = arr[..., 0]\n","    if arr.ndim == 3 and arr.shape[0] == 1:\n","        arr = arr[0]\n","    return arr.astype(np.float32)\n","\n","def _as_np_mask(m):\n","    \"\"\"Return a 2D numpy array for mask display.\"\"\"\n","    if torch.is_tensor(m):\n","        return m.detach().cpu().numpy().astype(np.int32)\n","    return np.asarray(m).astype(np.int32)\n","\n","def _ensure_img_ch(img_norm):\n","    \"\"\"Make image HxWx1 for Albumentations.\"\"\"\n","    return img_norm[..., None] if img_norm.ndim == 2 else img_norm\n","\n","def _safe_per_image_minmax(x, eps=1e-8):\n","    \"\"\"Fallback if per_image_minmax is not defined.\"\"\"\n","    x = x.astype(np.float32)\n","    mn = float(np.min(x))\n","    mx = float(np.max(x))\n","    if (mx - mn) < eps:\n","        return np.zeros_like(x, dtype=np.float32)\n","    return (x - mn) / (mx - mn)\n","\n","def _apply_transform_safely(tf, img_ch, mask_raw, seed=None):\n","    \"\"\"\n","    Apply Albumentations transform without permanently changing global RNG:\n","    - If fixed_rng(seed) exists: use it (restores RNG after).\n","    - Else: apply tf directly.\n","    \"\"\"\n","    if seed is not None and (\"fixed_rng\" in globals()) and callable(globals().get(\"fixed_rng\")):\n","        with fixed_rng(int(seed)):\n","            return tf(image=img_ch, mask=mask_raw)\n","    return tf(image=img_ch, mask=mask_raw)\n","\n","# -----------------------------\n","# Main visualisation\n","# -----------------------------\n","def show_examples(train_items, val_items_by_domain, train_tf, val_tf):\n","    # ---------- TRAIN ROWS ----------\n","    unique_train_domains = []\n","    for it in train_items:\n","        d = it.get(\"domain\", None)\n","        if d is None:\n","            continue\n","        if d not in unique_train_domains:\n","            unique_train_domains.append(d)\n","\n","    if len(unique_train_domains) > 0:\n","        n_rows = len(unique_train_domains)\n","        fig, axes = plt.subplots(n_rows, 4, figsize=(16, 4 * n_rows))\n","        if n_rows == 1:\n","            axes = np.expand_dims(axes, 0)\n","\n","        rng_vis = random.Random(int(GLOBAL_SEED) + 123)  # local RNG only\n","\n","        for r, d in enumerate(unique_train_domains):\n","            dom_items = [it for it in train_items if it.get(\"domain\", None) == d]\n","            if not dom_items:\n","                continue\n","            ex = rng_vis.choice(dom_items)\n","\n","            img_raw  = tiff.imread(str(ex[\"xct_path\"])).astype(np.float32)\n","            mask_raw = tiff.imread(str(ex[\"mask_path\"])).astype(np.int32)\n","\n","            if \"per_image_minmax\" in globals() and callable(globals().get(\"per_image_minmax\")):\n","                img_norm = per_image_minmax(img_raw)\n","            else:\n","                img_norm = _safe_per_image_minmax(img_raw)\n","\n","            img_vis = np.clip(img_norm, 0.0, 1.0)\n","            img_ch  = _ensure_img_ch(img_norm)\n","\n","            seed = int(GLOBAL_SEED) + 777 + r  # deterministic for viz only\n","            aug = _apply_transform_safely(train_tf, img_ch, mask_raw, seed=seed)\n","\n","            img_aug  = _as_np_img(aug[\"image\"])\n","            mask_aug = _as_np_mask(aug[\"mask\"])\n","\n","            axes[r, 0].imshow(img_vis, cmap=\"gray\", vmin=0, vmax=1)\n","            axes[r, 0].set_title(f\"{d} - train XCT (raw)\")\n","            axes[r, 0].axis(\"off\")\n","\n","            axes[r, 1].imshow(mask_raw, cmap=MASK_CMAP, norm=MASK_NORM)\n","            axes[r, 1].set_title(f\"{d} - train MASK (raw)\")\n","            axes[r, 1].axis(\"off\")\n","\n","            axes[r, 2].imshow(np.clip(img_aug, 0.0, 1.0), cmap=\"gray\", vmin=0, vmax=1)\n","            axes[r, 2].set_title(f\"{d} - train XCT (aug {PATCH_SIZE}√ó{PATCH_SIZE})\")\n","            axes[r, 2].axis(\"off\")\n","\n","            axes[r, 3].imshow(mask_aug, cmap=MASK_CMAP, norm=MASK_NORM)\n","            axes[r, 3].set_title(f\"{d} - train MASK (aug {PATCH_SIZE}√ó{PATCH_SIZE})\")\n","            axes[r, 3].axis(\"off\")\n","\n","            u = np.unique(mask_raw)\n","            if np.any((u < 0) | (u > 2)):\n","                print(f\"‚ö†Ô∏è  [{d}] raw mask has labels outside {{0,1,2}}: {u}\")\n","\n","        fig.suptitle(\"TRAIN: one row per domain (raw & augmented)\", fontsize=14)\n","        fig.tight_layout()\n","        plt.show()\n","\n","    # ---------- VAL ROWS ----------\n","    # Safe val domains source\n","    if \"VAL_DOMAINS\" in globals():\n","        val_domains = list(VAL_DOMAINS)\n","    else:\n","        val_domains = list(val_items_by_domain.keys())\n","\n","    unique_val_domains = [d for d in val_domains if len(val_items_by_domain.get(d, [])) > 0]\n","\n","    if len(unique_val_domains) > 0:\n","        n_rows = len(unique_val_domains)\n","        fig, axes = plt.subplots(n_rows, 4, figsize=(16, 4 * n_rows))\n","        if n_rows == 1:\n","            axes = np.expand_dims(axes, 0)\n","\n","        rng_vis = random.Random(int(GLOBAL_SEED) + 999)  # local RNG only\n","\n","        for r, d in enumerate(unique_val_domains):\n","            dom_items = val_items_by_domain.get(d, [])\n","            if not dom_items:\n","                continue\n","            ex = rng_vis.choice(dom_items)\n","\n","            img_raw  = tiff.imread(str(ex[\"xct_path\"])).astype(np.float32)\n","            mask_raw = tiff.imread(str(ex[\"mask_path\"])).astype(np.int32)\n","\n","            if \"per_image_minmax\" in globals() and callable(globals().get(\"per_image_minmax\")):\n","                img_norm = per_image_minmax(img_raw)\n","            else:\n","                img_norm = _safe_per_image_minmax(img_raw)\n","\n","            img_vis = np.clip(img_norm, 0.0, 1.0)\n","            img_ch  = _ensure_img_ch(img_norm)\n","\n","            seed = int(GLOBAL_SEED) + 888 + r\n","            aug = _apply_transform_safely(val_tf, img_ch, mask_raw, seed=seed)\n","\n","            img_aug  = _as_np_img(aug[\"image\"])\n","            mask_aug = _as_np_mask(aug[\"mask\"])\n","\n","            axes[r, 0].imshow(img_vis, cmap=\"gray\", vmin=0, vmax=1)\n","            axes[r, 0].set_title(f\"{d} - val XCT (raw)\")\n","            axes[r, 0].axis(\"off\")\n","\n","            axes[r, 1].imshow(mask_raw, cmap=MASK_CMAP, norm=MASK_NORM)\n","            axes[r, 1].set_title(f\"{d} - val MASK (raw)\")\n","            axes[r, 1].axis(\"off\")\n","\n","            axes[r, 2].imshow(np.clip(img_aug, 0.0, 1.0), cmap=\"gray\", vmin=0, vmax=1)\n","            axes[r, 2].set_title(f\"{d} - val XCT ({PATCH_SIZE}√ó{PATCH_SIZE})\")\n","            axes[r, 2].axis(\"off\")\n","\n","            axes[r, 3].imshow(mask_aug, cmap=MASK_CMAP, norm=MASK_NORM)\n","            axes[r, 3].set_title(f\"{d} - val MASK ({PATCH_SIZE}√ó{PATCH_SIZE})\")\n","            axes[r, 3].axis(\"off\")\n","\n","            u = np.unique(mask_raw)\n","            if np.any((u < 0) | (u > 2)):\n","                print(f\"‚ö†Ô∏è  [{d}] raw mask has labels outside {{0,1,2}}: {u}\")\n","\n","        fig.suptitle(\"VAL: one row per domain (raw & transformed)\", fontsize=14)\n","        fig.tight_layout()\n","        plt.show()\n","\n","# --------------------------\n","# Visual check now\n","# --------------------------\n","show_examples(train_items, val_items_by_domain, TRAIN_TRANSFORM, VAL_TRANSFORM)\n","\n","# ============================================================\n","# OPTIONAL: Local viz loader sanity check (Option B)\n","# - Only if train_loader does not exist yet.\n","# - Does NOT overwrite training globals.\n","# ============================================================\n","if \"train_loader\" not in globals():\n","    viz_ds = SegmentationDataset(train_items, transform=TRAIN_TRANSFORM, base_seed=int(GLOBAL_SEED))\n","    viz_ds.set_epoch(0)\n","\n","    viz_gen = torch.Generator()\n","    viz_gen.manual_seed(int(GLOBAL_SEED))\n","\n","    viz_loader = DataLoader(\n","        viz_ds,\n","        batch_size=BATCH_SIZE,\n","        shuffle=True,\n","        num_workers=0,\n","        pin_memory=True,\n","        worker_init_fn=worker_init_fn,\n","        generator=viz_gen,\n","    )\n","\n","    b = next(iter(viz_loader))\n","    xb, yb, db = viz_unpack_batch(b)\n","    dom_ex = list(sorted(set(list(db))))[:3] if db is not None else \"None\"\n","    print(\"üîé [viz_loader] Train batch shape:\", xb.shape, yb.shape, \"| domains example:\", dom_ex)\n","\n","else:\n","    b = next(iter(train_loader))\n","    xb, yb, db = viz_unpack_batch(b)\n","    dom_ex = list(sorted(set(list(db))))[:3] if db is not None else \"None\"\n","    print(\"üîé [train_loader] Train batch shape:\", xb.shape, yb.shape, \"| domains example:\", dom_ex)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gsz0HGzZhooh"},"outputs":[],"source":["# ============================================================\n","# CELL 3: Result folder + paths + Excel logger (Table A + Table B)\n","# ============================================================\n","import time\n","from pathlib import Path\n","import pandas as pd\n","import warnings\n","from openpyxl import load_workbook\n","\n","# --- Safety: required globals from previous cells ---\n","assert \"RESULTS_ROOT\"     in globals(), \"RESULTS_ROOT is not defined. Run Cell 1 first.\"\n","assert \"EXPERIMENT_NAME\"  in globals(), \"EXPERIMENT_NAME is not defined. Run Cell 2 first.\"\n","assert \"GLOBAL_SEED\"      in globals(), \"GLOBAL_SEED is not defined. Run Cell 2 first.\"\n","assert \"RUN_NAME\"         in globals(), \"RUN_NAME is not defined. Run Cell 2 first.\"\n","\n","warnings.simplefilter(\"ignore\", FutureWarning)\n","\n","# ------------------------------------------------------------\n","# Root where ALL training results will be stored\n","#   RESULTS_ROOT / <RUN_NAME> /\n","# ------------------------------------------------------------\n","TRAIN_ROOT = Path(RESULTS_ROOT)\n","TRAIN_ROOT.mkdir(parents=True, exist_ok=True)\n","\n","\n","\n","# For determinism debugging: avoid mixing old ckpts/logs with new runs\n","FORCE_FRESH_RUN_DIR = False  # final paper False\n","\n","if FORCE_FRESH_RUN_DIR:\n","    run_tag = time.strftime(\"%Y%m%d_%H%M%S\")\n","    RUN_DIRNAME = f\"{RUN_NAME}__{run_tag}\"\n","else:\n","    RUN_DIRNAME = RUN_NAME\n","\n","RUN_ROOT = TRAIN_ROOT / RUN_DIRNAME\n","RUN_ROOT.mkdir(parents=True, exist_ok=True)\n","\n","# ------------------------------------------------------------\n","# Model key ‚Üí prefix mapping (used for filenames)\n","# ------------------------------------------------------------\n","MODEL_KEY_TO_PREFIX = {\n","         \"unet\": \"UNet\",\n","    \"unet3plus\": \"UNet3Plus\",\n","    \"sslite_v1\": \"SSLite_V1_EnhSemiFull\",\n","    \"sslite_v2\": \"SSLite_V2_4Branch\",\n","    \"sslite_v3\": \"SSLite_V3_Baseline\",\n","}\n","SUPPORTED_MODEL_KEYS = list(MODEL_KEY_TO_PREFIX.keys())\n","\n","# Optional aliases accepted (to prevent path/log saving crashes)\n","MODEL_KEY_ALIASES = {\n","    \"unet3+\": \"unet3plus\",\n","    \"unet3plus\": \"unet3plus\",\n","    \"ssunet3pluslite_v1\": \"sslite_v1\",\n","    \"ssunet3pluslite-v1\": \"sslite_v1\",\n","    \"sslite_v1\": \"sslite_v1\",\n","    \"sslite_v2\": \"sslite_v2\",\n","    \"sslite_v3\": \"sslite_v3\",\n","    \"unet\": \"unet\",\n","}\n","\n","print(\"\\n‚úÖ Supported models for saving:\")\n","for k, p in MODEL_KEY_TO_PREFIX.items():\n","    print(f\"   ‚Ä¢ key='{k}'  ‚Üí prefix='{p}'\")\n","\n","# ------------------------------------------------------------\n","# Paths for a given model\n","# ------------------------------------------------------------\n","def get_model_paths(model_name: str):\n","    \"\"\"\n","    Return all important paths for a given model inside:\n","      RESULTS_ROOT / <RUN_NAME> /\n","\n","    model_name (key, case-insensitive) ‚àà {\n","        \"unet\", \"unet3plus\", \"sslite_v1\", \"sslite_v2\", \"sslite_v3\"\n","    }\n","    Aliases are also accepted and mapped to canonical keys.\n","    \"\"\"\n","    raw_key = str(model_name).lower()\n","    key = MODEL_KEY_ALIASES.get(raw_key, raw_key)\n","\n","    if key not in MODEL_KEY_TO_PREFIX:\n","        raise ValueError(\n","            f\"Unknown model_name='{model_name}'. Supported keys: {SUPPORTED_MODEL_KEYS}\"\n","        )\n","\n","    prefix = MODEL_KEY_TO_PREFIX[key]\n","    base = RUN_ROOT\n","\n","    return {\n","        \"base_dir\":  base,\n","        \"best_ckpt\": base / f\"{prefix}_best_model.pth\",\n","        \"last_ckpt\": base / f\"{prefix}_last_model.pth\",\n","        \"log_csv\":   base / f\"{prefix}_log.csv\",\n","        \"log_xlsx\":  base / f\"{prefix}_log.xlsx\",\n","    }\n","\n","# ------------------------------------------------------------\n","# Hard Dice/IoU columns (val-only in training cell, but always present here)\n","# NOTE: OUT_CLASSES assumed 3 for explicit columns as requested.\n","# ------------------------------------------------------------\n","HARD_KEYS = [\n","    \"hard_dice_macro\",\n","    \"hard_iou_macro\",\n","    \"hard_dice_c0\", \"hard_dice_c1\", \"hard_dice_c2\",\n","    \"hard_iou_c0\",  \"hard_iou_c1\",  \"hard_iou_c2\",\n","]\n","\n","# ------------------------------------------------------------\n","# Table B: epoch-wise result columns\n","# Notes:\n","#  - Soft dice/IoU (dice_d0 / iou_d0) REMOVED as requested.\n","#  - hard_* columns are expected for VAL (training cell may leave train rows empty).\n","# ------------------------------------------------------------\n","TABLE_B_COLUMNS = [\n","    \"phase\",          # \"train\" / \"val\"\n","    \"eval_domain\",    # typically \"ALL\"\n","    \"epoch\",\n","    \"model\",          # model key (e.g. \"sslite_v3\")\n","    \"lr\",\n","    \"time_sec\",\n","    \"loss_total\",\n","    \"loss_ce\",\n","    \"loss_dice\",\n","\n","    \"dice_domain\",    # string summary (optional)\n","    \"iou_domain\",     # string summary (optional)\n","    \"dice_classes\",   # string summary (optional)\n","    \"iou_classes\",    # string summary (optional)\n","\n","] + HARD_KEYS + [\n","\n","    # Run/meta toggles (filled from globals if row doesn't provide them)\n","    \"ce_mode\",        # \"ce\" / \"focal\"\n","    \"abl_fusion\",     # \"native\" / \"unet_skip\"\n","    \"use_bn\",         # bool (NEW: for BN ablation logging)\n","    \"use_gn\",         # bool\n","    \"base_ch\",        # int (e.g. 32/64)\n","    \"ds_on\",          # bool (deep supervision on/off)\n","]\n","\n","# ------------------------------------------------------------\n","# Internal: build Table A row (RUN_META + model + LR schedule summary)\n","# ------------------------------------------------------------\n","def _build_tableA_row(model_name: str):\n","    \"\"\"\n","    Build a single dict for Table A (one wide row).\n","    Uses RUN_META if available, plus model & LR schedule info.\n","    \"\"\"\n","    rowA = {}\n","\n","    # RUN_META: experiment, run_name, seed, domain counts, etc. (from Cell 2)\n","    if \"RUN_META\" in globals():\n","        rowA.update(RUN_META)\n","\n","    # Ensure some basic fields exist\n","    rowA.setdefault(\"experiment\", EXPERIMENT_NAME)\n","    rowA.setdefault(\"run_name\",   RUN_NAME)          # logical name (e.g., C1_seed42)\n","    rowA.setdefault(\"seed\",       int(GLOBAL_SEED))\n","\n","    # physical folder trace\n","    rowA[\"run_dir\"]      = str(RUN_ROOT.name)        # e.g., C1_seed42__20260112_103550\n","    rowA[\"run_dir_full\"] = str(RUN_ROOT)             # full path\n","    rowA[\"results_root\"] = str(TRAIN_ROOT)           # full path/root\n","\n","\n","    raw_key = str(model_name).lower()\n","    key = MODEL_KEY_ALIASES.get(raw_key, raw_key)\n","\n","    rowA[\"model_key\"]    = key\n","    rowA[\"model_prefix\"] = MODEL_KEY_TO_PREFIX.get(key, key)\n","\n","    # LR schedule summary (if those globals exist ‚Äì from later cells)\n","    init_lr         = globals().get(\"INIT_LR\", None)\n","    decay_factor    = globals().get(\"LR_DECAY_FACTOR\", None)\n","    lr_patience     = globals().get(\"LR_PATIENCE_EPOCHS\", None)\n","    min_lr          = globals().get(\"MIN_LR\", None)\n","    early_stop_pat  = globals().get(\"EARLY_STOP_PATIENCE\", None)\n","\n","    lr_parts = []\n","    if init_lr is not None:\n","        lr_parts.append(f\"init={init_lr}\")\n","    if decay_factor is not None and lr_patience is not None:\n","        lr_parts.append(f\"/{decay_factor} after {lr_patience} no-improve\")\n","    if min_lr is not None:\n","        lr_parts.append(f\"min={min_lr}\")\n","    if early_stop_pat is not None:\n","        lr_parts.append(f\"early_stop={early_stop_pat}\")\n","\n","    rowA[\"lr_schedule\"] = \"; \".join(lr_parts) if lr_parts else \"\"\n","    return rowA\n","\n","# ------------------------------------------------------------\n","# Internal: ensure Excel + CSV have Table A + Table B header\n","# ------------------------------------------------------------\n","def _ensure_log_tables_initialized(model_name: str):\n","    \"\"\"\n","    Create <Prefix>_log.xlsx and <Prefix>_log.csv with:\n","      - Table A: 1 row at the top\n","      - Blank row\n","      - Table B: header row for epoch-wise metrics\n","    If files already exist, do nothing (we append only).\n","    \"\"\"\n","    paths     = get_model_paths(model_name)\n","    csv_path  = paths[\"log_csv\"]\n","    xlsx_path = paths[\"log_xlsx\"]\n","\n","    # If both files exist, assume structure is already created.\n","    if xlsx_path.exists() and csv_path.exists():\n","        return\n","\n","\n","    rowA = _build_tableA_row(model_name)\n","    dfA  = pd.DataFrame([rowA])\n","\n","    # Excel: Table A + empty row + Table B header\n","    with pd.ExcelWriter(xlsx_path, engine=\"openpyxl\") as writer:\n","        dfA.to_excel(writer, index=False, sheet_name=\"Log\", startrow=0)\n","        start_row_B = len(dfA) + 2\n","        pd.DataFrame(columns=TABLE_B_COLUMNS).to_excel(\n","            writer, index=False, sheet_name=\"Log\", startrow=start_row_B\n","        )\n","\n","    # CSV: same structure\n","    with open(csv_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(\",\".join(dfA.columns) + \"\\n\")\n","        f.write(\",\".join(str(rowA.get(col, \"\")) for col in dfA.columns) + \"\\n\")\n","        f.write(\"\\n\")\n","        f.write(\",\".join(TABLE_B_COLUMNS) + \"\\n\")\n","\n","# ------------------------------------------------------------\n","# Public: append one epoch row to Table B (both CSV and XLSX)\n","# ------------------------------------------------------------\n","def append_log_row(model_name: str, row: dict):\n","    \"\"\"\n","    Append a single row (dict) to Table B of:\n","      <Prefix>_log.csv / <Prefix>_log.xlsx\n","\n","    Notes:\n","      - Table A is written once (RUN_META, model, LR schedule).\n","      - Table B rows are appended under the header.\n","      - Soft dice/IoU columns are REMOVED.\n","      - hard_* columns are expected for VAL (train rows can be empty strings).\n","\n","    Expected keys in 'row' (Table B):\n","      phase, eval_domain, epoch, model, lr, time_sec,\n","      loss_total, loss_ce, loss_dice,\n","      dice_domain (str), iou_domain (str),\n","      dice_classes (str), iou_classes (str),\n","      hard_dice_macro, hard_iou_macro, hard_dice_c*, hard_iou_c*,\n","      ce_mode, abl_fusion, use_bn, use_gn, base_ch, ds_on\n","    \"\"\"\n","    with warnings.catch_warnings():\n","        warnings.simplefilter(\"ignore\", FutureWarning)\n","\n","        _ensure_log_tables_initialized(model_name)\n","\n","        paths     = get_model_paths(model_name)\n","        csv_path  = paths[\"log_csv\"]\n","        xlsx_path = paths[\"log_xlsx\"]\n","\n","        # Fill hard keys (optional) if missing\n","        for k in HARD_KEYS:\n","            row.setdefault(k, \"\")\n","\n","        # Fill meta from globals if not explicitly provided\n","        row.setdefault(\"ce_mode\",    globals().get(\"CE_MODE\", None))\n","        row.setdefault(\"abl_fusion\", globals().get(\"ABL_FUSION\", None))\n","        row.setdefault(\"use_bn\",     globals().get(\"USE_BN\", None))\n","        row.setdefault(\"use_gn\",     globals().get(\"USE_GN\", None))\n","        row.setdefault(\"base_ch\",    globals().get(\"BASE_CH\", None))\n","\n","        raw_key = str(model_name).lower()\n","        key = MODEL_KEY_ALIASES.get(raw_key, raw_key)\n","        if key == \"unet3plus\":\n","            _ds_default = globals().get(\"UNET3PLUS_DEEP_SUPERVISION\", None)\n","        elif key.startswith(\"sslite_\"):\n","            _ds_default = globals().get(\"SSLITE_DEEP_SUPERVISION\", None)\n","        else:\n","            _ds_default = None\n","        row.setdefault(\"ds_on\", _ds_default)\n","\n","        # Build rowB with consistent column order\n","        rowB = {col: row.get(col, None) for col in TABLE_B_COLUMNS}\n","\n","        # Round floats to 5 decimals where appropriate\n","        float_cols = [\n","            \"lr\", \"time_sec\",\n","            \"loss_total\", \"loss_ce\", \"loss_dice\",\n","\n","            # hard metrics\n","            \"hard_dice_macro\", \"hard_iou_macro\",\n","            \"hard_dice_c0\", \"hard_dice_c1\", \"hard_dice_c2\",\n","            \"hard_iou_c0\",  \"hard_iou_c1\",  \"hard_iou_c2\",\n","        ]\n","\n","        for col in float_cols:\n","            v = rowB.get(col, None)\n","            if v is not None and v != \"\":\n","                try:\n","                    rowB[col] = float(f\"{float(v):.5f}\")\n","                except Exception:\n","                    pass\n","\n","        # Append to CSV (Table B only)\n","        pd.DataFrame([rowB]).to_csv(csv_path, mode=\"a\", header=False, index=False)\n","\n","        # Append to XLSX\n","        wb = load_workbook(xlsx_path)\n","        ws = wb[\"Log\"]\n","        ws.append([rowB.get(col) for col in TABLE_B_COLUMNS])\n","        wb.save(xlsx_path)\n","\n","# ------------------------------------------------------------\n","# One-time, plain output structure print (easy to verify paths)\n","# ------------------------------------------------------------\n","print(\"\\nüìå Results folders\")\n","print(\"   RESULTS_ROOT:\", TRAIN_ROOT)\n","print(\"   RUN_ROOT    :\", RUN_ROOT)\n","\n","root_name = TRAIN_ROOT.name\n","print(\"\\nExpected files (per model) will appear like this:\")\n","print(f\"{root_name}/\")\n","print(f\"‚îî‚îÄ‚îÄ {RUN_ROOT.name}/\")   # <-- FIX: show actual physical folder\n","for key, prefix in MODEL_KEY_TO_PREFIX.items():\n","    print(f\"    ‚îú‚îÄ‚îÄ {prefix}_best_model.pth\")\n","    print(f\"    ‚îú‚îÄ‚îÄ {prefix}_last_model.pth\")\n","    print(f\"    ‚îú‚îÄ‚îÄ {prefix}_log.csv\")\n","    print(f\"    ‚îî‚îÄ‚îÄ {prefix}_log.xlsx\")\n","    break\n","\n","print(\"\\n‚úÖ Cell 3 ready: get_model_paths(...) and append_log_row(...) are available.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E_w0_PeYmc2K"},"outputs":[],"source":["# ============================================================\n","# CELL 4: Model architectures (official-style + SS-UNet3+ Lite)\n","#   - UNet          (Ronneberger et al. 2015)  [NO BN/GN]\n","#   - UNet3Plus     (UNet 3+, Huang et al. 2020; full-scale fusion)\n","#   - SSUNet3PlusLite_V1  (Enhanced Semi-Full, 5 inputs/level)\n","#   - SSUNet3PlusLite_V2  (Updated Semi-Full, 4 inputs/level)\n","#   - SSUNet3PlusLite_V3  (Baseline, current setup)\n","#\n","# Upsampling control (GLOBAL, for ALL models):\n","#   - Set UPSAMPLE_POLICY in Cell 4.5A:\n","#       \"nearest\" | \"bilinear\"\n","#\n","# NOTE (fusion ablation for SS-Lite):\n","#   - ABL_FUSION = \"native\" | \"unet_skip\"\n","# ============================================================\n","\n","!pip -q install thop\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# ------------------------------------------------------------\n","# Unified resize policy for ALL models (nearest | bilinear)\n","# ------------------------------------------------------------\n","if \"UPSAMPLE_POLICY\" not in globals():\n","    UPSAMPLE_POLICY = \"nearest\"  # safe default if Cell 4.5A not run\n","\n","\n","def _get_policy() -> str:\n","    return str(globals().get(\"UPSAMPLE_POLICY\", \"nearest\")).lower()\n","\n","\n","def _interp(x: torch.Tensor, size, mode: str) -> torch.Tensor:\n","    mode = mode.lower()\n","    if mode in (\"nearest\", \"area\", \"nearest-exact\"):\n","        return F.interpolate(x, size=size, mode=mode)\n","    return F.interpolate(x, size=size, mode=mode, align_corners=False)\n","\n","def _resize_like(x: torch.Tensor, ref: torch.Tensor, mode: str = None) -> torch.Tensor:\n","    \"\"\"\n","    Resize x to match ref spatial size using GLOBAL policy:\n","      - nearest  : nearest interpolation\n","      - bilinear : bilinear interpolation\n","    \"\"\"\n","    th, tw = ref.shape[-2:]\n","    h, w = x.shape[-2:]\n","    if (h, w) == (th, tw):\n","        return x\n","\n","    policy = (mode or _get_policy()).lower()\n","    if policy not in (\"nearest\", \"bilinear\", \"area\", \"nearest-exact\"):\n","        policy = \"nearest\"\n","\n","    return _interp(x, size=(th, tw), mode=policy)\n","\n","\n","def _resize_to(x: torch.Tensor, ref: torch.Tensor, mode: str = None) -> torch.Tensor:\n","    return _resize_like(x, ref, mode=mode)\n","\n","# ------------------------------------------------------------\n","# Global config: deep supervision toggle for UNet3Plus\n","# ------------------------------------------------------------\n","if \"UNET3PLUS_DEEP_SUPERVISION\" not in globals():\n","    UNET3PLUS_DEEP_SUPERVISION = True\n","\n","# ------------------------------------------------------------\n","# Normalization factory (ablation-friendly)\n","# ------------------------------------------------------------\n","def make_norm(num_channels: int) -> nn.Module:\n","    if \"NORM_LAYER_2D\" in globals():\n","        layer = globals()[\"NORM_LAYER_2D\"]\n","        if callable(layer):\n","            return layer(num_channels)\n","\n","    if \"NORM_LAYER\" in globals():\n","        layer = globals()[\"NORM_LAYER\"]\n","        if callable(layer):\n","            return layer(num_channels)\n","\n","    use_gn = bool(globals().get(\"USE_GN\", False))\n","    use_bn = bool(globals().get(\"USE_BN\", True))\n","\n","    if (not use_gn) and (not use_bn):\n","        return nn.Identity()\n","\n","    if use_gn:\n","        groups_cfg = int(globals().get(\"GN_GROUPS\", 8))\n","        groups = max(1, min(groups_cfg, num_channels))\n","        while groups > 1 and (num_channels % groups != 0):\n","            groups -= 1\n","        if num_channels % groups == 0 and groups > 0:\n","            return nn.GroupNorm(num_groups=groups, num_channels=num_channels)\n","        return nn.BatchNorm2d(num_channels) if use_bn else nn.Identity()\n","\n","    return nn.BatchNorm2d(num_channels) if use_bn else nn.Identity()\n","\n","# ------------------------------------------------------------\n","# Basic building blocks (UNet3Plus, SS-Lite use these)\n","# ------------------------------------------------------------\n","class DoubleConv(nn.Module):\n","    \"\"\"(Conv ‚Üí Norm ‚Üí ReLU) √ó 2\"\"\"\n","    def __init__(self, in_ch: int, out_ch: int):\n","        super().__init__()\n","        self.block = nn.Sequential(\n","            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n","            make_norm(out_ch),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n","            make_norm(out_ch),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","    def forward(self, x):\n","        return self.block(x)\n","\n","class OutConv(nn.Module):\n","    \"\"\"1√ó1 conv to logits\"\"\"\n","    def __init__(self, in_ch: int, out_ch: int):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","# ------------------------------------------------------------\n","# UNet blocks (NO BN / NO GN) ‚Äî official-style\n","# ------------------------------------------------------------\n","class DoubleConv_UNet(nn.Module):\n","    \"\"\"(Conv -> ReLU) x2 (no normalization).\"\"\"\n","    def __init__(self, in_ch: int, out_ch: int):\n","        super().__init__()\n","        self.block = nn.Sequential(\n","            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=True),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=True),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","    def forward(self, x):\n","        return self.block(x)\n","\n","class Down_UNet(nn.Module):\n","    \"\"\"MaxPool(2) -> DoubleConv_UNet\"\"\"\n","    def __init__(self, in_ch: int, out_ch: int):\n","        super().__init__()\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv = DoubleConv_UNet(in_ch, out_ch)\n","\n","    def forward(self, x):\n","        return self.conv(self.pool(x))\n","\n","class Up_UNet(nn.Module):\n","    \"\"\"\n","    UNet up block: resize (GLOBAL policy) -> concat -> DoubleConv_UNet.\n","    Controlled ONLY by UPSAMPLE_POLICY.\n","    \"\"\"\n","    def __init__(self, in_ch: int, skip_ch: int, out_ch: int):\n","        super().__init__()\n","        self.conv = DoubleConv_UNet(in_ch + skip_ch, out_ch)\n","\n","    def forward(self, x, skip):\n","        x = _resize_like(x, skip)\n","        x = torch.cat([skip, x], dim=1)\n","        return self.conv(x)\n","\n","# ------------------------------------------------------------\n","# 1) UNet (Ronneberger et al. 2015) ‚Äî NO BN / NO GN\n","# ------------------------------------------------------------\n","class UNet(nn.Module):\n","    \"\"\"\n","    Original U-Net (Ronneberger et al. 2015), depth=5.\n","    Upsampling controlled ONLY by UPSAMPLE_POLICY.\n","    \"\"\"\n","    def __init__(self, in_channels: int = 1, num_classes: int = 3, base_ch: int = 32, **_ignored):\n","        super().__init__()\n","        c1, c2, c3, c4, c5 = base_ch, base_ch*2, base_ch*4, base_ch*8, base_ch*16\n","\n","        self.inc   = DoubleConv_UNet(in_channels, c1)\n","        self.down1 = Down_UNet(c1, c2)\n","        self.down2 = Down_UNet(c2, c3)\n","        self.down3 = Down_UNet(c3, c4)\n","        self.down4 = Down_UNet(c4, c5)\n","\n","        self.up1 = Up_UNet(c5, c4, c4)\n","        self.up2 = Up_UNet(c4, c3, c3)\n","        self.up3 = Up_UNet(c3, c2, c2)\n","        self.up4 = Up_UNet(c2, c1, c1)\n","\n","        self.outc = OutConv(c1, num_classes)\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","\n","        x = self.up1(x5, x4)\n","        x = self.up2(x,  x3)\n","        x = self.up3(x,  x2)\n","        x = self.up4(x,  x1)\n","        return self.outc(x)\n","\n","# ------------------------------------------------------------\n","# 2) UNet3Plus (full-scale multi-scale fusion, official style)\n","# ------------------------------------------------------------\n","class UNet3Plus(nn.Module):\n","    \"\"\"\n","    UNet 3+ with full-scale skip connections and optional deep supervision.\n","    Resizing honors UPSAMPLE_POLICY via _resize_like/_resize_to.\n","    \"\"\"\n","    def __init__(self, in_channels: int = 1, num_classes: int = 3, base_ch: int = 32, deep_supervision=None):\n","        super().__init__()\n","        if deep_supervision is None:\n","            deep_supervision = bool(globals().get(\"UNET3PLUS_DEEP_SUPERVISION\", True))\n","        self.deep_supervision = deep_supervision\n","\n","        f1, f2, f3, f4, f5 = base_ch, base_ch*2, base_ch*4, base_ch*8, base_ch*16\n","        self.enc0 = DoubleConv(in_channels, f1); self.pool0 = nn.MaxPool2d(2)\n","        self.enc1 = DoubleConv(f1, f2);          self.pool1 = nn.MaxPool2d(2)\n","        self.enc2 = DoubleConv(f2, f3);          self.pool2 = nn.MaxPool2d(2)\n","        self.enc3 = DoubleConv(f3, f4);          self.pool3 = nn.MaxPool2d(2)\n","        self.enc4 = DoubleConv(f4, f5)\n","\n","        cat_ch = base_ch\n","        dec_ch = base_ch * 5\n","        self.proj_e0 = nn.Conv2d(f1, cat_ch, 1)\n","        self.proj_e1 = nn.Conv2d(f2, cat_ch, 1)\n","        self.proj_e2 = nn.Conv2d(f3, cat_ch, 1)\n","        self.proj_e3 = nn.Conv2d(f4, cat_ch, 1)\n","        self.proj_e4 = nn.Conv2d(f5, cat_ch, 1)\n","\n","        self.proj_d4 = nn.Conv2d(dec_ch, cat_ch, 1)\n","        self.proj_d3 = nn.Conv2d(dec_ch, cat_ch, 1)\n","        self.proj_d2 = nn.Conv2d(dec_ch, cat_ch, 1)\n","        self.proj_d1 = nn.Conv2d(dec_ch, cat_ch, 1)\n","\n","        self.conv_d4 = DoubleConv(cat_ch*5, dec_ch)\n","        self.conv_d3 = DoubleConv(cat_ch*6, dec_ch)\n","        self.conv_d2 = DoubleConv(cat_ch*7, dec_ch)\n","        self.conv_d1 = DoubleConv(cat_ch*8, dec_ch)\n","        self.conv_d0 = DoubleConv(cat_ch*9, dec_ch)\n","\n","        if self.deep_supervision:\n","            self.out_d4 = nn.Conv2d(dec_ch, num_classes, 1)\n","            self.out_d3 = nn.Conv2d(dec_ch, num_classes, 1)\n","            self.out_d2 = nn.Conv2d(dec_ch, num_classes, 1)\n","            self.out_d1 = nn.Conv2d(dec_ch, num_classes, 1)\n","            self.out_d0 = nn.Conv2d(dec_ch, num_classes, 1)\n","        else:\n","            self.out_d0 = nn.Conv2d(dec_ch, num_classes, 1)\n","\n","    @staticmethod\n","    def _proj_resize(x, proj_layer, ref):\n","        x = proj_layer(x)\n","        x = _resize_like(x, ref)\n","        return x\n","\n","    def forward(self, x):\n","        e0 = self.enc0(x)\n","        e1 = self.enc1(self.pool0(e0))\n","        e2 = self.enc2(self.pool1(e1))\n","        e3 = self.enc3(self.pool2(e2))\n","        e4 = self.enc4(self.pool3(e3))\n","\n","        ref4 = e4\n","        d4 = self.conv_d4(torch.cat([\n","            self._proj_resize(e0, self.proj_e0, ref4),\n","            self._proj_resize(e1, self.proj_e1, ref4),\n","            self._proj_resize(e2, self.proj_e2, ref4),\n","            self._proj_resize(e3, self.proj_e3, ref4),\n","            self._proj_resize(e4, self.proj_e4, ref4),\n","        ], dim=1))\n","\n","        ref3 = e3\n","        d3 = self.conv_d3(torch.cat([\n","            self._proj_resize(e0, self.proj_e0, ref3),\n","            self._proj_resize(e1, self.proj_e1, ref3),\n","            self._proj_resize(e2, self.proj_e2, ref3),\n","            self._proj_resize(e3, self.proj_e3, ref3),\n","            self._proj_resize(e4, self.proj_e4, ref3),\n","            self._proj_resize(d4, self.proj_d4, ref3),\n","        ], dim=1))\n","\n","        ref2 = e2\n","        d2 = self.conv_d2(torch.cat([\n","            self._proj_resize(e0, self.proj_e0, ref2),\n","            self._proj_resize(e1, self.proj_e1, ref2),\n","            self._proj_resize(e2, self.proj_e2, ref2),\n","            self._proj_resize(e3, self.proj_e3, ref2),\n","            self._proj_resize(e4, self.proj_e4, ref2),\n","            self._proj_resize(d3, self.proj_d3, ref2),\n","            self._proj_resize(d4, self.proj_d4, ref2),\n","        ], dim=1))\n","\n","        ref1 = e1\n","        d1 = self.conv_d1(torch.cat([\n","            self._proj_resize(e0, self.proj_e0, ref1),\n","            self._proj_resize(e1, self.proj_e1, ref1),\n","            self._proj_resize(e2, self.proj_e2, ref1),\n","            self._proj_resize(e3, self.proj_e3, ref1),\n","            self._proj_resize(e4, self.proj_e4, ref1),\n","            self._proj_resize(d2, self.proj_d2, ref1),\n","            self._proj_resize(d3, self.proj_d3, ref1),\n","            self._proj_resize(d4, self.proj_d4, ref1),\n","        ], dim=1))\n","\n","        ref0 = e0\n","        d0 = self.conv_d0(torch.cat([\n","            self._proj_resize(e0, self.proj_e0, ref0),\n","            self._proj_resize(e1, self.proj_e1, ref0),\n","            self._proj_resize(e2, self.proj_e2, ref0),\n","            self._proj_resize(e3, self.proj_e3, ref0),\n","            self._proj_resize(e4, self.proj_e4, ref0),\n","            self._proj_resize(d1, self.proj_d1, ref0),\n","            self._proj_resize(d2, self.proj_d2, ref0),\n","            self._proj_resize(d3, self.proj_d3, ref0),\n","            self._proj_resize(d4, self.proj_d4, ref0),\n","        ], dim=1))\n","\n","        if self.deep_supervision:\n","            y0 = self.out_d0(d0)\n","            y1 = self.out_d1(d1)\n","            y2 = self.out_d2(d2)\n","            y3 = self.out_d3(d3)\n","            y4 = self.out_d4(d4)\n","\n","            # resize to input size using GLOBAL policy\n","            ref_full = x\n","            y1 = _resize_like(y1, ref_full)\n","            y2 = _resize_like(y2, ref_full)\n","            y3 = _resize_like(y3, ref_full)\n","            y4 = _resize_like(y4, ref_full)\n","            return [y0, y1, y2, y3, y4]\n","\n","        return self.out_d0(d0)\n","\n","# ------------------------------------------------------------\n","# 3) SS-UNet3+ Lite FAST (decoder variants V1‚ÄìV3)\n","# ------------------------------------------------------------\n","class LiteBlock(nn.Module):\n","    \"\"\"1x1 -> depthwise 3x3 -> 1x1 (with make_norm)\"\"\"\n","    def __init__(self, in_ch, out_ch, expansion=1.0):\n","        super().__init__()\n","        hidden = max(1, int(in_ch * expansion))\n","        self.pw1 = nn.Conv2d(in_ch, hidden, 1, bias=False); self.bn1 = make_norm(hidden)\n","        self.dw  = nn.Conv2d(hidden, hidden, 3, padding=1, groups=hidden, bias=False); self.bn2 = make_norm(hidden)\n","        self.pw2 = nn.Conv2d(hidden, out_ch, 1, bias=False); self.bn3 = make_norm(out_ch)\n","        self.act = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        x = self.act(self.bn1(self.pw1(x)))\n","        x = self.act(self.bn2(self.dw(x)))\n","        x = self.act(self.bn3(self.pw2(x)))\n","        return x\n","\n","class ScaleSelectiveFusion(nn.Module):\n","    \"\"\"\n","    Projection + Norm + ReLU per branch, then mean().\n","    Resizing uses GLOBAL policy via _resize_like.\n","    \"\"\"\n","    def __init__(self, in_ch_list, cat_ch):\n","        super().__init__()\n","        self.proj = nn.ModuleList([nn.Conv2d(c, cat_ch, 1, bias=False) for c in in_ch_list])\n","        self.bn   = nn.ModuleList([make_norm(cat_ch) for _ in in_ch_list])\n","\n","    def forward(self, features):\n","        proj_feats = []\n","        ref = None\n","        for f, conv, bn in zip(features, self.proj, self.bn):\n","            p = F.relu(bn(conv(f)))\n","            if ref is None:\n","                ref = p\n","            else:\n","                p = _resize_like(p, ref)\n","            proj_feats.append(p)\n","        return torch.stack(proj_feats, dim=0).mean(dim=0)\n","\n","class SSUNet3PlusLiteBase(nn.Module):\n","    def __init__(self, in_channels, num_classes, base_ch=32, deep_supervision=True,\n","                 ds_w0=0.5, ds_w1=0.15, ds_w2=0.15, ds_w3=0.10, ds_w4=0.10):\n","        super().__init__()\n","        self.deep_supervision = deep_supervision\n","\n","        w = [float(ds_w0), float(ds_w1), float(ds_w2), float(ds_w3), float(ds_w4)]\n","        s = sum(w)\n","        if s <= 0:\n","            w = [0.5, 0.15, 0.15, 0.10, 0.10]; s = sum(w)\n","        self.ds_w0, self.ds_w1, self.ds_w2, self.ds_w3, self.ds_w4 = [wi/s for wi in w]\n","\n","        f1, f2, f3, f4, f5 = base_ch, base_ch*2, base_ch*4, base_ch*8, base_ch*16\n","        self.f1, self.f2, self.f3, self.f4, self.f5 = f1, f2, f3, f4, f5\n","\n","        self.enc0 = LiteBlock(in_channels, f1, 1.0); self.pool0 = nn.MaxPool2d(2)\n","        self.enc1 = LiteBlock(f1, f2, 1.0);         self.pool1 = nn.MaxPool2d(2)\n","        self.enc2 = LiteBlock(f2, f3, 1.0);         self.pool2 = nn.MaxPool2d(2)\n","        self.enc3 = LiteBlock(f3, f4, 1.0);         self.pool3 = nn.MaxPool2d(2)\n","        self.enc4 = LiteBlock(f4, f5, 1.0)\n","\n","        self.cat_ch = base_ch\n","        self.dec_ch = base_ch * 4\n","\n","        self.out0 = nn.Conv2d(self.dec_ch, num_classes, 1)\n","        if self.deep_supervision:\n","            self.out1 = nn.Conv2d(self.dec_ch, num_classes, 1)\n","            self.out2 = nn.Conv2d(self.dec_ch, num_classes, 1)\n","            self.out3 = nn.Conv2d(self.dec_ch, num_classes, 1)\n","            self.out4 = nn.Conv2d(self.dec_ch, num_classes, 1)\n","\n","    def _encode(self, x):\n","        e0 = self.enc0(x)\n","        e1 = self.enc1(self.pool0(e0))\n","        e2 = self.enc2(self.pool1(e1))\n","        e3 = self.enc3(self.pool2(e2))\n","        e4 = self.enc4(self.pool3(e3))\n","        return e0, e1, e2, e3, e4\n","\n","    def _fuse_logits(self, x, d0, d1, d2, d3, d4):\n","        \"\"\"\n","        If DS OFF -> return y0 tensor.\n","        If DS ON:\n","          - SSLITE_DS_OUTPUT=\"fused\" -> return fused tensor\n","          - SSLITE_DS_OUTPUT=\"list\"  -> return [y0,y1,y2,y3,y4]\n","        \"\"\"\n","        y0 = self.out0(d0)\n","        if not self.deep_supervision:\n","            return y0\n","\n","        ref = d0\n","        y1 = self.out1(_resize_to(d1, ref))\n","        y2 = self.out2(_resize_to(d2, ref))\n","        y3 = self.out3(_resize_to(d3, ref))\n","        y4 = self.out4(_resize_to(d4, ref))\n","\n","        ds_mode = str(globals().get(\"SSLITE_DS_OUTPUT\", \"fused\")).lower()\n","        if ds_mode == \"list\":\n","            return [y0, y1, y2, y3, y4]\n","\n","        return (self.ds_w0*y0 + self.ds_w1*y1 + self.ds_w2*y2 + self.ds_w3*y3 + self.ds_w4*y4)\n","\n","# ---------------- Variant V1: Enhanced Semi-Full (5 inputs each) ------------\n","class SSUNet3PlusLite_V1(SSUNet3PlusLiteBase):\n","    def __init__(self, in_channels, num_classes, base_ch=32, deep_supervision=True,\n","                 ds_w0=0.5, ds_w1=0.15, ds_w2=0.15, ds_w3=0.10, ds_w4=0.10):\n","        super().__init__(in_channels, num_classes, base_ch, deep_supervision, ds_w0, ds_w1, ds_w2, ds_w3, ds_w4)\n","        f1,f2,f3,f4,f5 = self.f1,self.f2,self.f3,self.f4,self.f5\n","        cat_ch, dec_ch = self.cat_ch, self.dec_ch\n","        self.fuse4 = ScaleSelectiveFusion([f1,f2,f3,f4,f5], cat_ch);             self.dec4 = LiteBlock(cat_ch, dec_ch, 1.0)\n","        self.fuse3 = ScaleSelectiveFusion([f2,f3,f4,f5,dec_ch], cat_ch);         self.dec3 = LiteBlock(cat_ch, dec_ch, 1.0)\n","        self.fuse2 = ScaleSelectiveFusion([f1,f2,f3,f4,dec_ch], cat_ch);         self.dec2 = LiteBlock(cat_ch, dec_ch, 1.0)\n","        self.fuse1 = ScaleSelectiveFusion([f1,f2,f3,dec_ch,dec_ch], cat_ch);     self.dec1 = LiteBlock(cat_ch, dec_ch, 1.0)\n","        self.fuse0 = ScaleSelectiveFusion([f1,f2,dec_ch,dec_ch,dec_ch], cat_ch); self.dec0 = LiteBlock(cat_ch, dec_ch, 1.0)\n","\n","    def forward(self, x):\n","        e0,e1,e2,e3,e4 = self._encode(x)\n","        use_unet_skip = (str(globals().get(\"ABL_FUSION\",\"native\")).lower() == \"unet_skip\")\n","        def _keep_only(lst, keep_idx):\n","            return lst if not use_unet_skip else [t if i in keep_idx else torch.zeros_like(t) for i,t in enumerate(lst)]\n","\n","        ref4 = e4\n","        d4 = self.dec4(self.fuse4(_keep_only([_resize_to(e0,ref4),_resize_to(e1,ref4),_resize_to(e2,ref4),_resize_to(e3,ref4),_resize_to(e4,ref4)], [4])))\n","\n","        ref3 = e3\n","        d3 = self.dec3(self.fuse3(_keep_only([_resize_to(e1,ref3),_resize_to(e2,ref3),_resize_to(e3,ref3),_resize_to(e4,ref3),_resize_to(d4,ref3)], [2,4])))\n","\n","        ref2 = e2\n","        d2 = self.dec2(self.fuse2(_keep_only([_resize_to(e0,ref2),_resize_to(e1,ref2),_resize_to(e2,ref2),_resize_to(e3,ref2),_resize_to(d3,ref2)], [2,4])))\n","\n","        ref1 = e1\n","        d1 = self.dec1(self.fuse1(_keep_only([_resize_to(e0,ref1),_resize_to(e1,ref1),_resize_to(e2,ref1),_resize_to(d2,ref1),_resize_to(d3,ref1)], [1,3])))\n","\n","        ref0 = e0\n","        d0 = self.dec0(self.fuse0(_keep_only([_resize_to(e0,ref0),_resize_to(e1,ref0),_resize_to(d1,ref0),_resize_to(d2,ref0),_resize_to(d3,ref0)], [0,2])))\n","\n","        return self._fuse_logits(x, d0, d1, d2, d3, d4)\n","\n","# ---------------- Variant V2: Updated Semi-Full (4 inputs each) ------------\n","class SSUNet3PlusLite_V2(SSUNet3PlusLiteBase):\n","    def __init__(self, in_channels, num_classes, base_ch=32, deep_supervision=True,\n","                 ds_w0=0.5, ds_w1=0.15, ds_w2=0.15, ds_w3=0.10, ds_w4=0.10):\n","        super().__init__(in_channels, num_classes, base_ch, deep_supervision, ds_w0, ds_w1, ds_w2, ds_w3, ds_w4)\n","        f1,f2,f3,f4,f5 = self.f1,self.f2,self.f3,self.f4,self.f5\n","        cat_ch, dec_ch = self.cat_ch, self.dec_ch\n","        self.fuse4 = ScaleSelectiveFusion([f2,f3,f4,f5], cat_ch);         self.dec4 = LiteBlock(cat_ch, dec_ch, 1.0)\n","        self.fuse3 = ScaleSelectiveFusion([f2,f3,f4,dec_ch], cat_ch);     self.dec3 = LiteBlock(cat_ch, dec_ch, 1.0)\n","        self.fuse2 = ScaleSelectiveFusion([f1,f2,f3,dec_ch], cat_ch);     self.dec2 = LiteBlock(cat_ch, dec_ch, 1.0)\n","        self.fuse1 = ScaleSelectiveFusion([f1,f2,dec_ch,dec_ch], cat_ch); self.dec1 = LiteBlock(cat_ch, dec_ch, 1.0)\n","        self.fuse0 = ScaleSelectiveFusion([f1,f2,dec_ch,dec_ch], cat_ch); self.dec0 = LiteBlock(cat_ch, dec_ch, 1.0)\n","\n","    def forward(self, x):\n","        e0,e1,e2,e3,e4 = self._encode(x)\n","        use_unet_skip = (str(globals().get(\"ABL_FUSION\",\"native\")).lower() == \"unet_skip\")\n","        def _keep_only(lst, keep_idx):\n","            return lst if not use_unet_skip else [t if i in keep_idx else torch.zeros_like(t) for i,t in enumerate(lst)]\n","\n","        ref4 = e4\n","        d4 = self.dec4(self.fuse4(_keep_only([_resize_to(e1,ref4),_resize_to(e2,ref4),_resize_to(e3,ref4),_resize_to(e4,ref4)], [3])))\n","\n","        ref3 = e3\n","        d3 = self.dec3(self.fuse3(_keep_only([_resize_to(e1,ref3),_resize_to(e2,ref3),_resize_to(e3,ref3),_resize_to(d4,ref3)], [2,3])))\n","\n","        ref2 = e2\n","        d2 = self.dec2(self.fuse2(_keep_only([_resize_to(e0,ref2),_resize_to(e1,ref2),_resize_to(e2,ref2),_resize_to(d3,ref2)], [2,3])))\n","\n","        ref1 = e1\n","        d1 = self.dec1(self.fuse1(_keep_only([_resize_to(e0,ref1),_resize_to(e1,ref1),_resize_to(d2,ref1),_resize_to(d3,ref1)], [1,2])))\n","\n","        ref0 = e0\n","        d0 = self.dec0(self.fuse0(_keep_only([_resize_to(e0,ref0),_resize_to(e1,ref0),_resize_to(d1,ref0),_resize_to(d2,ref0)], [0,2])))\n","\n","        return self._fuse_logits(x, d0, d1, d2, d3, d4)\n","\n","# ---------------- Variant V3: Baseline (current setup) ----------------------\n","class SSUNet3PlusLite_V3(SSUNet3PlusLiteBase):\n","    def __init__(self, in_channels, num_classes, base_ch=32, deep_supervision=True,\n","                 ds_w0=0.5, ds_w1=0.15, ds_w2=0.15, ds_w3=0.10, ds_w4=0.10):\n","        super().__init__(in_channels, num_classes, base_ch, deep_supervision, ds_w0, ds_w1, ds_w2, ds_w3, ds_w4)\n","        f1,f2,f3,f4,f5 = self.f1,self.f2,self.f3,self.f4,self.f5\n","        cat_ch, dec_ch = self.cat_ch, self.dec_ch\n","        self.fuse4 = ScaleSelectiveFusion([f3,f4,f5], cat_ch);             self.dec4 = LiteBlock(cat_ch, dec_ch, 1.0)\n","        self.fuse3 = ScaleSelectiveFusion([f2,f3,f4,dec_ch], cat_ch);     self.dec3 = LiteBlock(cat_ch, dec_ch, 1.0)\n","        self.fuse2 = ScaleSelectiveFusion([f1,f2,f3,dec_ch], cat_ch);     self.dec2 = LiteBlock(cat_ch, dec_ch, 1.0)\n","        self.fuse1 = ScaleSelectiveFusion([f1,f2,dec_ch], cat_ch);        self.dec1 = LiteBlock(cat_ch, dec_ch, 1.0)\n","        self.fuse0 = ScaleSelectiveFusion([f1,f2,dec_ch,dec_ch], cat_ch); self.dec0 = LiteBlock(cat_ch, dec_ch, 1.0)\n","\n","    def forward(self, x):\n","        e0,e1,e2,e3,e4 = self._encode(x)\n","        use_unet_skip = (str(globals().get(\"ABL_FUSION\",\"native\")).lower() == \"unet_skip\")\n","        def _keep_only(lst, keep_idx):\n","            return lst if not use_unet_skip else [t if i in keep_idx else torch.zeros_like(t) for i,t in enumerate(lst)]\n","\n","        ref4 = e4\n","        d4 = self.dec4(self.fuse4(_keep_only([_resize_to(e2,ref4),_resize_to(e3,ref4),_resize_to(e4,ref4)], [2])))\n","\n","        ref3 = e3\n","        d3 = self.dec3(self.fuse3(_keep_only([_resize_to(e1,ref3),_resize_to(e2,ref3),_resize_to(e3,ref3),_resize_to(d4,ref3)], [2,3])))\n","\n","        ref2 = e2\n","        d2 = self.dec2(self.fuse2(_keep_only([_resize_to(e0,ref2),_resize_to(e1,ref2),_resize_to(e2,ref2),_resize_to(d3,ref2)], [2,3])))\n","\n","        ref1 = e1\n","        d1 = self.dec1(self.fuse1(_keep_only([_resize_to(e0,ref1),_resize_to(e1,ref1),_resize_to(d2,ref1)], [1,2])))\n","\n","        ref0 = e0\n","        d0 = self.dec0(self.fuse0(_keep_only([_resize_to(e0,ref0),_resize_to(e1,ref0),_resize_to(d1,ref0),_resize_to(d2,ref0)], [0,2])))\n","\n","        return self._fuse_logits(x, d0, d1, d2, d3, d4)\n","\n","# ------------------------------------------------------------\n","# Helper factories for Lite models (V1‚ÄìV3 only)\n","# ------------------------------------------------------------\n","def create_sslite_v1(num_classes: int):\n","    in_ch = globals().get(\"IN_CHANNELS\", 1)\n","    base_ch_global = globals().get(\"BASE_CH\", 32)\n","    ds_on = bool(globals().get(\"SSLITE_DEEP_SUPERVISION\", True))\n","    w = globals().get(\"DS_WEIGHTS\", [0.5,0.15,0.15,0.10,0.10])\n","    if not isinstance(w, (list, tuple)): w = [0.5,0.15,0.15,0.10,0.10]\n","    w = (list(w) + [0,0,0,0,0])[:5]\n","    return SSUNet3PlusLite_V1(in_ch, num_classes, base_ch_global, ds_on, w[0],w[1],w[2],w[3],w[4])\n","\n","def create_sslite_v2(num_classes: int):\n","    in_ch = globals().get(\"IN_CHANNELS\", 1)\n","    base_ch_global = globals().get(\"BASE_CH\", 32)\n","    ds_on = bool(globals().get(\"SSLITE_DEEP_SUPERVISION\", True))\n","    w = globals().get(\"DS_WEIGHTS\", [0.5,0.15,0.15,0.10,0.10])\n","    if not isinstance(w, (list, tuple)): w = [0.5,0.15,0.15,0.10,0.10]\n","    w = (list(w) + [0,0,0,0,0])[:5]\n","    return SSUNet3PlusLite_V2(in_ch, num_classes, base_ch_global, ds_on, w[0],w[1],w[2],w[3],w[4])\n","\n","def create_sslite_v3(num_classes: int):\n","    in_ch = globals().get(\"IN_CHANNELS\", 1)\n","    base_ch_global = globals().get(\"BASE_CH\", 32)\n","    ds_on = bool(globals().get(\"SSLITE_DEEP_SUPERVISION\", True))\n","    w = globals().get(\"DS_WEIGHTS\", [0.5,0.15,0.15,0.10,0.10])\n","    if not isinstance(w, (list, tuple)): w = [0.5,0.15,0.15,0.10,0.10]\n","    w = (list(w) + [0,0,0,0,0])[:5]\n","    return SSUNet3PlusLite_V3(in_ch, num_classes, base_ch_global, ds_on, w[0],w[1],w[2],w[3],w[4])\n","\n","# ------------------------------------------------------------\n","# Helper: parameter counting + FLOPs/MACs summary\n","# ------------------------------------------------------------\n","def count_parameters(model: nn.Module) -> int:\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","class _ThopSafeWrapper(nn.Module):\n","    def __init__(self, model: nn.Module):\n","        super().__init__()\n","        self.model = model\n","    def forward(self, x):\n","        y = self.model(x)\n","        return y[0] if isinstance(y, (list, tuple)) else y\n","\n","def _profile_macs_flops(model: nn.Module, x: torch.Tensor):\n","    from thop import profile\n","    model.eval()\n","    safe_model = _ThopSafeWrapper(model)\n","    with torch.no_grad():\n","        macs, _ = profile(safe_model, inputs=(x,), verbose=False)\n","    macs = float(macs)\n","    flops = 2.0 * macs\n","    return macs, flops\n","\n","def print_model_param_flops_summary(in_channels=1, num_classes=3, base_ch=32, input_hw=256, device=None):\n","    if device is None:\n","        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","    models = dict(\n","        UNet=UNet(in_channels=in_channels, num_classes=num_classes, base_ch=base_ch),\n","        UNet3Plus=UNet3Plus(in_channels=in_channels, num_classes=num_classes, base_ch=base_ch),\n","        SSLite_V1_EnhSemiFull=SSUNet3PlusLite_V1(in_channels, num_classes, base_ch, deep_supervision=bool(globals().get(\"SSLITE_DEEP_SUPERVISION\", True))),\n","        SSLite_V2_4Branch=SSUNet3PlusLite_V2(in_channels, num_classes, base_ch, deep_supervision=bool(globals().get(\"SSLITE_DEEP_SUPERVISION\", True))),\n","        SSLite_V3_Baseline=SSUNet3PlusLite_V3(in_channels, num_classes, base_ch, deep_supervision=bool(globals().get(\"SSLITE_DEEP_SUPERVISION\", True))),\n","    )\n","\n","    x = torch.randn(1, in_channels, input_hw, input_hw, device=device)\n","\n","    print(\"\\nüìä Params + FLOPs (trainable):\")\n","    print(f\"   Config: in_channels={in_channels}, num_classes={num_classes}, base_ch={base_ch}, input=1x{in_channels}x{input_hw}x{input_hw}\")\n","    print(f\"   UPSAMPLE_POLICY = '{_get_policy()}' (nearest | bilinear)\")\n","    print(f\"   UNET3PLUS_DEEP_SUPERVISION = {globals().get('UNET3PLUS_DEEP_SUPERVISION', True)}\")\n","    print(f\"   SSLITE_DS_OUTPUT = '{globals().get('SSLITE_DS_OUTPUT','fused')}'\")\n","    print(\"\")\n","\n","    for name, model in models.items():\n","        model = model.to(device)\n","        n_params = count_parameters(model)\n","        macs, flops = _profile_macs_flops(model, x)\n","        print(f\"   {name:<26}: {n_params:10,d} params (~{n_params/1e6:6.3f} M) | FLOPs ~ {flops/1e9:7.3f} G | MACs {macs/1e9:7.3f} G\")\n","\n","print(\"‚úÖ Models defined: UNet, UNet3Plus, SSUNet3PlusLite (V1‚ÄìV3)\")\n","print_model_param_flops_summary(in_channels=1, num_classes=3, base_ch=32, input_hw=256)\n"]},{"cell_type":"markdown","metadata":{"id":"JXCCYhdTdZWt"},"source":["## Ablation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGTlIZQvFdIy"},"outputs":[],"source":["# ============================================================\n","# CELL 4.5A: Ablation / Config switchboard (toggles only)\n","# ============================================================\n","\n","# ---------------------------\n","# Upsampling policy (GLOBAL, for ALL models)\n","# ---------------------------\n","#   \"nearest\"  : F.interpolate(..., mode=\"nearest\")\n","#   \"bilinear\" : F.interpolate(..., mode=\"bilinear\", align_corners=False)\n","#\n","\n","UPSAMPLE_POLICY = \"nearest\"  # \"nearest\" | \"bilinear\"\n","\n","\n","# ---------------------------\n","# UNet3Plus deep supervision output\n","# ---------------------------\n","UNET3PLUS_DEEP_SUPERVISION = True  # True -> returns [y0,y1,y2,y3,y4], False -> returns y0 only\n","\n","# ---------------------------\n","# SS-UNet3+ Lite deep supervision\n","# ---------------------------\n","SSLITE_DEEP_SUPERVISION = True\n","\n","# What SS-Lite returns when DS is ON:\n","#   \"fused\" -> returns fused logits tensor (default behavior in your Lite models)\n","#   \"list\"  -> returns [y0,y1,y2,y3,y4] like UNet3Plus\n","SSLITE_DS_OUTPUT = \"fused\"  # \"fused\" | \"list\"\n","\n","# Deep supervision weights for SS-Lite (used when SSLITE_DS_OUTPUT=\"fused\")\n","DS_WEIGHTS = [0.8, 0.05, 0.05, 0.05, 0.05]  # [y0, y1, y2, y3, y4]\n","\n","# Normalize DS weights safely\n","_ds_sum = float(sum(DS_WEIGHTS))\n","_ds_has_neg = any(float(w) < 0.0 for w in DS_WEIGHTS)\n","if _ds_has_neg or _ds_sum <= 0.0:\n","    print(\"‚ö†Ô∏è  Warning: DS_WEIGHTS invalid (negatives or sum<=0). Using safe defaults.\")\n","    DS_WEIGHTS = [0.5, 0.15, 0.15, 0.10, 0.10]\n","else:\n","    DS_WEIGHTS = [float(w) / _ds_sum for w in DS_WEIGHTS]\n","\n","# ---------------------------\n","# Normalization ablation (affects UNet3Plus + SS-Lite blocks that use make_norm)\n","# ---------------------------\n","# - USE_GN=True  -> GroupNorm\n","# - USE_GN=False + USE_BN=True  -> BatchNorm\n","# - USE_GN=False + USE_BN=False -> NO normalization (Identity)\n","USE_GN    = False\n","USE_BN    = True\n","GN_GROUPS = 16\n","\n","# ---------------------------\n","# Width ablation\n","# ---------------------------\n","BASE_CH = 32  # e.g., 32 or 64\n","\n","# ---------------------------\n","# Fusion ablation (SS-UNet3+ Lite V1‚ÄìV3 only)\n","# ---------------------------\n","#   \"native\"    -> original version-specific fusion (default)\n","#   \"unet_skip\" -> UNet-like same-level skip only (zero out unused inputs)\n","ABL_FUSION = \"native\"  # \"native\" | \"unet_skip\"\n","\n","# ---------------------------\n","# Loss config (if you use hybrid loss elsewhere)\n","# ---------------------------\n","CE_WEIGHT   = 0.5\n","DICE_WEIGHT = 0.5\n","\n","\n","# ---------------------------\n","# Quick print\n","# ---------------------------\n","print(\"\\n‚úÖ Ablation / config summary\")\n","print(f\"   UPSAMPLE_POLICY           = '{UPSAMPLE_POLICY}'  (nearest | bilinear)\")\n","print(f\"   BASE_CH                   = {BASE_CH}\")\n","print(f\"   USE_GN                    = {USE_GN} (GN_GROUPS={GN_GROUPS})\")\n","print(f\"   USE_BN                    = {USE_BN}\")\n","print(f\"   UNET3PLUS_DEEP_SUPERVISION= {UNET3PLUS_DEEP_SUPERVISION}\")\n","print(f\"   SSLITE_DEEP_SUPERVISION   = {SSLITE_DEEP_SUPERVISION}\")\n","print(f\"   SSLITE_DS_OUTPUT          = '{SSLITE_DS_OUTPUT}'  (fused | list)\")\n","print(f\"   DS_WEIGHTS (y0..y4)       = {DS_WEIGHTS}\")\n","print(f\"   ABL_FUSION                = '{ABL_FUSION}'\")\n","print(f\"   CE_WEIGHT / DICE_WEIGHT   = {CE_WEIGHT} / {DICE_WEIGHT}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_zMdcafIm1X7"},"outputs":[],"source":["# ============================================================\n","# CELL 4.5B: Training helpers (metrics, epoch runner, printing)\n","#   - merge_logits: handle deep supervision lists\n","#   - Deterministic CE for segmentation (avoids CUDA nll_loss2d kernel)\n","#   - SOFT Dice/IoU sums (for dice loss + soft reporting)\n","#   - HARD Dice/IoU from tp/fp/fn (for tqdm + early stopping + reporting)\n","#   - _unpack_batch: support dict / tuple batches\n","#   - _run_epoch_core: shared TRAIN/VAL one-epoch runner\n","#   - run_epoch_train / run_epoch_val (wrappers + backward-compatible aliases)\n","#   - format_domain_metrics, format_class_metrics, print_domain_stats\n","#\n","# Returns dict keys expected by Cell 5 training loop:\n","#   time_sec, loss_total, loss_ce, loss_dice,\n","#   dice_macro, iou_macro, dice_per_class, iou_per_class,          (SOFT)\n","#   hard_dice_macro, hard_iou_macro, hard_dice_per_class, hard_iou_per_class,\n","#   domain_stats (optional): per-domain tp/fp/fn tensors\n","# ============================================================\n","\n","import time\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","\n","\n","# ------------------------------------------------------------\n","# Deep supervision helper\n","# ------------------------------------------------------------\n","def merge_logits(logits_any):\n","    \"\"\"\n","    If model outputs deep supervision as list/tuple of logits, use the MAIN full-res head.\n","\n","    For your implementations:\n","      - UNet3Plus (deep_supervision=True) returns [y0, y1, y2, y3, y4]\n","        where y0 is the full-resolution D0 head (main output).\n","      - SS-Lite models normally return a single fused tensor when SSLITE_DS_OUTPUT=\"fused\".\n","    \"\"\"\n","    if isinstance(logits_any, (list, tuple)):\n","        # IMPORTANT: use index 0 (y0, full-res), NOT the last element (y4)\n","        return logits_any[0]\n","    return logits_any\n","\n","\n","def _ensure_logits_size(logits: torch.Tensor, y: torch.Tensor):\n","    if logits.shape[-2:] == y.shape[-2:]:\n","        return logits\n","    mode = str(globals().get(\"UPSAMPLE_POLICY\", \"nearest\")).lower()\n","    if mode not in (\"nearest\", \"bilinear\", \"nearest-exact\", \"area\"):\n","        mode = \"nearest\"\n","    if mode in (\"nearest\", \"area\", \"nearest-exact\"):\n","        return F.interpolate(logits, size=y.shape[-2:], mode=mode)\n","    return F.interpolate(logits, size=y.shape[-2:], mode=\"bilinear\", align_corners=False)\n","\n","\n","\n","# ------------------------------------------------------------\n","# Deterministic Cross Entropy (segmentation)\n","#   - Avoids CUDA nll_loss2d_forward_out_* which may be nondeterministic\n","#     under torch.use_deterministic_algorithms(True)\n","# ------------------------------------------------------------\n","def cross_entropy_det_2d(\n","    logits: torch.Tensor,\n","    targets: torch.Tensor,\n","    weight: torch.Tensor = None,\n","    ignore_index: int = -100,\n","    label_smoothing: float = 0.0,\n","    reduction: str = \"mean\",\n","):\n","    \"\"\"\n","    Deterministic replacement for F.cross_entropy for segmentation.\n","\n","    logits:  (B,C,H,W)\n","    targets: (B,H,W)\n","    \"\"\"\n","    logits = logits.float()\n","    targets = targets.long()\n","\n","    logp = F.log_softmax(logits, dim=1)  # (B,C,H,W)\n","\n","    if ignore_index is None:\n","        valid = torch.ones_like(targets, dtype=torch.bool)\n","    else:\n","        valid = (targets != ignore_index)\n","\n","    # gather requires valid indices; clamp ignored pixels to 0 safely\n","    gather_t = targets.clone()\n","    gather_t[~valid] = 0\n","\n","    nll = -logp.gather(1, gather_t.unsqueeze(1)).squeeze(1)  # (B,H,W)\n","\n","    if weight is not None:\n","        w = weight.to(logits.device, dtype=logits.dtype)\n","        nll = nll * w[gather_t]\n","\n","    if label_smoothing and label_smoothing > 0.0:\n","        smooth = -logp.mean(dim=1)  # (B,H,W)\n","        nll = (1.0 - label_smoothing) * nll + label_smoothing * smooth\n","\n","    nll = nll[valid]\n","\n","    if reduction == \"mean\":\n","        return nll.mean() if nll.numel() > 0 else torch.tensor(0.0, device=logits.device, dtype=logits.dtype)\n","    if reduction == \"sum\":\n","        return nll.sum()\n","    if reduction == \"none\":\n","        out = torch.zeros_like(targets, dtype=logits.dtype)\n","        out[valid] = nll\n","        return out\n","    raise ValueError(f\"Unknown reduction: {reduction}\")\n","\n","\n","# ------------------------------------------------------------\n","# SOFT Dice/IoU (for LOSS + optional soft reporting)\n","# ------------------------------------------------------------\n","def _soft_sums_from_logits(logits: torch.Tensor, targets: torch.Tensor, num_classes: int):\n","    \"\"\"\n","    Return per-class sums:\n","      inter_c, dice_denom_c, iou_union_c  (shape C)\n","    \"\"\"\n","    logits = _ensure_logits_size(logits, targets)\n","    probs = torch.softmax(logits, dim=1)  # (B,C,H,W)\n","    onehot = F.one_hot(targets.long(), num_classes=num_classes).permute(0, 3, 1, 2).float()\n","    dims = (0, 2, 3)\n","\n","    inter = (probs * onehot).sum(dims)\n","    dice_denom = (probs + onehot).sum(dims)\n","    iou_union = (probs + onehot - probs * onehot).sum(dims)\n","    return inter, dice_denom, iou_union\n","\n","\n","def _soft_metrics_from_sums(inter, dice_denom, iou_union, eps=1e-6):\n","    dice_pc = (2.0 * inter + eps) / (dice_denom + eps)\n","    iou_pc  = (inter + eps) / (iou_union + eps)\n","    dice_m  = dice_pc.mean()\n","    iou_m   = iou_pc.mean()\n","    return dice_pc, dice_m, iou_pc, iou_m\n","\n","\n","# Backward-compatible helper name used in older parts/comments\n","def _metric_from_sums(inter, dice_denom, iou_union, eps=1e-6):\n","    \"\"\"Return (dice_per_class, iou_per_class) from accumulated sums.\"\"\"\n","    dice_pc = (2.0 * inter + eps) / (dice_denom + eps)\n","    iou_pc  = (inter + eps) / (iou_union + eps)\n","    return dice_pc, iou_pc\n","\n","\n","# ------------------------------------------------------------\n","# HARD metrics (tp/fp/fn) for early stopping + tqdm\n","# ------------------------------------------------------------\n","@torch.no_grad()\n","def _hard_counts_from_logits(logits: torch.Tensor, targets: torch.Tensor, num_classes: int):\n","    logits = _ensure_logits_size(logits, targets)\n","    pred = torch.argmax(logits, dim=1)  # (B,H,W)\n","\n","    tp = torch.zeros(num_classes, device=logits.device, dtype=torch.float32)\n","    fp = torch.zeros(num_classes, device=logits.device, dtype=torch.float32)\n","    fn = torch.zeros(num_classes, device=logits.device, dtype=torch.float32)\n","\n","    for c in range(num_classes):\n","        p = (pred == c)\n","        t = (targets == c)\n","        tp[c] += (p & t).sum().float()\n","        fp[c] += (p & (~t)).sum().float()\n","        fn[c] += ((~p) & t).sum().float()\n","\n","    return tp, fp, fn\n","\n","\n","def _hard_metrics_from_counts(tp, fp, fn, eps=1e-6):\n","    \"\"\"\n","    Must exist because Cell 5 uses it directly for domain strings.\n","    Returns: dice_pc, dice_macro, iou_pc, iou_macro\n","    \"\"\"\n","    dice_pc = (2.0 * tp + eps) / (2.0 * tp + fp + fn + eps)\n","    iou_pc  = (tp + eps) / (tp + fp + fn + eps)\n","    dice_m  = dice_pc.mean()\n","    iou_m   = iou_pc.mean()\n","    return dice_pc, dice_m, iou_pc, iou_m\n","\n","\n","# ------------------------------------------------------------\n","# Batch unpack helper\n","# ------------------------------------------------------------\n","def _unpack_batch(batch):\n","    \"\"\"\n","    Support:\n","      - dict: {\"image\":..., \"mask\":..., \"domain\":...}\n","      - tuple/list: (x, y) or (x, y, domain)\n","    \"\"\"\n","    if isinstance(batch, dict):\n","        x = batch.get(\"image\", None)\n","        y = batch.get(\"mask\", None)\n","        dom = batch.get(\"domain\", None)\n","        return x, y, dom\n","\n","    if isinstance(batch, (list, tuple)):\n","        if len(batch) == 2:\n","            return batch[0], batch[1], None\n","        if len(batch) >= 3:\n","            return batch[0], batch[1], batch[2]\n","\n","    raise ValueError(\"Unsupported batch format\")\n","\n","\n","# ------------------------------------------------------------\n","# DS-aware CE + Dice loss (deterministic)\n","# ------------------------------------------------------------\n","def _ce_dice_loss_single(\n","    logits: torch.Tensor,\n","    targets: torch.Tensor,\n","    num_classes: int,\n","    ce_weight: float,\n","    dice_weight: float,\n","    dice_eps: float = 1e-6,\n","    ce_class_weight: torch.Tensor = None,\n","    ce_ignore_index: int = -100,\n","    ce_label_smoothing: float = 0.0,\n","):\n","    logits = _ensure_logits_size(logits, targets)\n","\n","    loss_ce = cross_entropy_det_2d(\n","        logits, targets,\n","        weight=ce_class_weight,\n","        ignore_index=ce_ignore_index,\n","        label_smoothing=ce_label_smoothing,\n","        reduction=\"mean\",\n","    )\n","\n","    inter, denom, union = _soft_sums_from_logits(logits, targets, num_classes=num_classes)\n","    dice_pc, dice_m, _, _ = _soft_metrics_from_sums(inter, denom, union, eps=dice_eps)\n","    loss_dice = 1.0 - dice_m\n","\n","    loss_total = float(ce_weight) * loss_ce + float(dice_weight) * loss_dice\n","    return loss_total, loss_ce, loss_dice\n","\n","\n","def loss_any(\n","    logits_any,\n","    targets,\n","    num_classes: int,\n","    ce_weight: float,\n","    dice_weight: float,\n","    dice_eps: float = 1e-6,\n","    ds_weights=None,\n","    ce_class_weight: torch.Tensor = None,\n","    ce_ignore_index: int = -100,\n","    ce_label_smoothing: float = 0.0,\n","):\n","    \"\"\"\n","    If logits_any is deep supervision list/tuple:\n","      loss = sum_i w_i * (CE + Dice) over outputs\n","    Otherwise:\n","      loss = CE + Dice\n","    Returns: (loss_total, loss_ce, loss_dice) as tensors\n","    \"\"\"\n","    if isinstance(logits_any, (list, tuple)):\n","        outs = list(logits_any)\n","        k = len(outs)\n","\n","        if ds_weights is None:\n","            ds_weights = globals().get(\"DS_WEIGHTS\", None)\n","        if ds_weights is None:\n","            ds_weights = [1.0] * k\n","        assert len(ds_weights) == k, \"ds_weights length mismatch\"\n","\n","        # normalize weights (stable scaling)\n","        s = float(sum(float(w) for w in ds_weights))\n","        ws = [(float(w) / s) for w in ds_weights] if s > 0 else [1.0 / k] * k\n","\n","        total = 0.0\n","        ce_acc = 0.0\n","        dice_acc = 0.0\n","        for w, lg in zip(ws, outs):\n","            lt, lce, ld = _ce_dice_loss_single(\n","                lg, targets,\n","                num_classes=num_classes,\n","                ce_weight=ce_weight,\n","                dice_weight=dice_weight,\n","                dice_eps=dice_eps,\n","                ce_class_weight=ce_class_weight,\n","                ce_ignore_index=ce_ignore_index,\n","                ce_label_smoothing=ce_label_smoothing,\n","            )\n","            total = total + w * lt\n","            ce_acc = ce_acc + w * lce\n","            dice_acc = dice_acc + w * ld\n","\n","        return total, ce_acc, dice_acc\n","\n","    return _ce_dice_loss_single(\n","        logits_any, targets,\n","        num_classes=num_classes,\n","        ce_weight=ce_weight,\n","        dice_weight=dice_weight,\n","        dice_eps=dice_eps,\n","        ce_class_weight=ce_class_weight,\n","        ce_ignore_index=ce_ignore_index,\n","        ce_label_smoothing=ce_label_smoothing,\n","    )\n","\n","\n","# ------------------------------------------------------------\n","# Shared one-epoch runner (TRAIN or VAL)\n","# ------------------------------------------------------------\n","def _run_epoch_core(\n","    model,\n","    loader,\n","    optimizer=None,\n","    train_mode: bool = True,\n","    device=None,\n","    num_classes: int = None,\n","    ce_weight: float = 1.0,\n","    dice_weight: float = 1.0,\n","    dice_eps: float = 1e-6,\n","    ds_weights=None,\n","    track_by_domain: bool = False,\n","    show_tqdm: bool = True,\n","    tqdm_desc: str = \"\",\n","    ce_class_weight: torch.Tensor = None,\n","    ce_ignore_index: int = -100,\n","    ce_label_smoothing: float = 0.0,\n","):\n","    \"\"\"\n","    Returns dict with keys expected by Cell 5 training loop:\n","      time_sec, loss_total, loss_ce, loss_dice,\n","      dice_macro, iou_macro, dice_per_class, iou_per_class,               (SOFT)\n","      hard_dice_macro, hard_iou_macro, hard_dice_per_class, hard_iou_per_class,\n","      domain_stats (optional): per-domain tp/fp/fn\n","    \"\"\"\n","    if device is None:\n","        device = globals().get(\"DEVICE\", None)\n","    if device is None:\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    if num_classes is None:\n","        num_classes = globals().get(\"NUM_CLASSES\", None)\n","    if num_classes is None:\n","        raise ValueError(\"NUM_CLASSES is not defined. Define it before running training.\")\n","\n","    if train_mode:\n","        model.train()\n","    else:\n","        model.eval()\n","\n","    use_amp = bool(globals().get(\"USE_AMP\", False)) if train_mode else False\n","    scaler = globals().get(\"SCALER\", None)\n","\n","    # accumulators (sample-weighted means)\n","    n_sum = 0\n","    loss_total_sum = 0.0\n","    loss_ce_sum = 0.0\n","    loss_dice_sum = 0.0\n","\n","    # SOFT sums (exact soft dice/iou)\n","    soft_inter_sum = torch.zeros(num_classes, device=device, dtype=torch.float32)\n","    soft_denom_sum = torch.zeros(num_classes, device=device, dtype=torch.float32)\n","    soft_union_sum = torch.zeros(num_classes, device=device, dtype=torch.float32)\n","\n","    # HARD sums (tp/fp/fn)\n","    tp_sum = torch.zeros(num_classes, device=device, dtype=torch.float32)\n","    fp_sum = torch.zeros(num_classes, device=device, dtype=torch.float32)\n","    fn_sum = torch.zeros(num_classes, device=device, dtype=torch.float32)\n","\n","    # Domain stats (HARD only)\n","    domain_stats = {}\n","\n","    t0 = time.perf_counter()\n","    it = loader\n","    if show_tqdm:\n","        it = tqdm(loader, desc=tqdm_desc, leave=False)\n","\n","    for batch in it:\n","        x, y, domains = _unpack_batch(batch)\n","        x = x.to(device, non_blocking=False)\n","        y = y.to(device, non_blocking=False)\n","\n","        if train_mode:\n","            optimizer.zero_grad(set_to_none=True)\n","\n","        # forward + loss\n","        if train_mode and use_amp:\n","            with torch.cuda.amp.autocast(enabled=True):\n","                logits_any = model(x)\n","                lt, lce, ld = loss_any(\n","                    logits_any, y,\n","                    num_classes=num_classes,\n","                    ce_weight=ce_weight,\n","                    dice_weight=dice_weight,\n","                    dice_eps=dice_eps,\n","                    ds_weights=ds_weights,\n","                    ce_class_weight=ce_class_weight,\n","                    ce_ignore_index=ce_ignore_index,\n","                    ce_label_smoothing=ce_label_smoothing,\n","                )\n","        else:\n","            logits_any = model(x)\n","            lt, lce, ld = loss_any(\n","                logits_any, y,\n","                num_classes=num_classes,\n","                ce_weight=ce_weight,\n","                dice_weight=dice_weight,\n","                dice_eps=dice_eps,\n","                ds_weights=ds_weights,\n","                ce_class_weight=ce_class_weight,\n","                ce_ignore_index=ce_ignore_index,\n","                ce_label_smoothing=ce_label_smoothing,\n","            )\n","\n","        if train_mode:\n","            if use_amp and (scaler is not None):\n","                scaler.scale(lt).backward()\n","                scaler.step(optimizer)\n","                scaler.update()\n","            else:\n","                lt.backward()\n","                optimizer.step()\n","\n","        bs = int(x.shape[0])\n","        n_sum += bs\n","        loss_total_sum += float(lt.item()) * bs\n","        loss_ce_sum    += float(lce.item()) * bs\n","        loss_dice_sum  += float(ld.item()) * bs\n","\n","        # metrics from final/highest-res logits\n","        logits = merge_logits(logits_any)\n","        logits = _ensure_logits_size(logits, y)\n","\n","        # SOFT sums\n","        inter_b, denom_b, union_b = _soft_sums_from_logits(logits, y, num_classes=num_classes)\n","        soft_inter_sum += inter_b\n","        soft_denom_sum += denom_b\n","        soft_union_sum += union_b\n","\n","        # HARD counts\n","        tp_b, fp_b, fn_b = _hard_counts_from_logits(logits, y, num_classes=num_classes)\n","        tp_sum += tp_b\n","        fp_sum += fp_b\n","        fn_sum += fn_b\n","\n","        # per-domain HARD\n","        if track_by_domain and (domains is not None):\n","            dom_list = list(domains)\n","            for dom in sorted(set(dom_list)):  # deterministic order\n","                idxs = [i for i, d in enumerate(dom_list) if d == dom]\n","                if not idxs:\n","                    continue\n","                lg_dom = logits[idxs]\n","                y_dom  = y[idxs]\n","                tp_d, fp_d, fn_d = _hard_counts_from_logits(lg_dom, y_dom, num_classes=num_classes)\n","\n","                if dom not in domain_stats:\n","                    domain_stats[dom] = {\n","                        \"n\": 0,\n","                        \"tp\": torch.zeros(num_classes, device=device, dtype=torch.float32),\n","                        \"fp\": torch.zeros(num_classes, device=device, dtype=torch.float32),\n","                        \"fn\": torch.zeros(num_classes, device=device, dtype=torch.float32),\n","                    }\n","                domain_stats[dom][\"n\"]  += len(idxs)\n","                domain_stats[dom][\"tp\"] += tp_d\n","                domain_stats[dom][\"fp\"] += fp_d\n","                domain_stats[dom][\"fn\"] += fn_d\n","\n","        # tqdm uses HARD macro (stable + matches epoch-end HARD)\n","        if show_tqdm:\n","            _, hd_m, _, hi_m = _hard_metrics_from_counts(tp_sum, fp_sum, fn_sum)\n","            it.set_postfix({\n","                \"loss\": loss_total_sum / max(1, n_sum),\n","                \"hdice\": float(hd_m),\n","                \"hiou\": float(hi_m),\n","            })\n","\n","    t1 = time.perf_counter()\n","\n","    # finalize SOFT metrics\n","    dice_pc, dice_m, iou_pc, iou_m = _soft_metrics_from_sums(\n","        soft_inter_sum, soft_denom_sum, soft_union_sum, eps=dice_eps\n","    )\n","\n","    # finalize HARD metrics\n","    hard_dice_pc, hard_dice_m, hard_iou_pc, hard_iou_m = _hard_metrics_from_counts(tp_sum, fp_sum, fn_sum)\n","\n","    out = {\n","        \"time_sec\": float(t1 - t0),\n","\n","        \"loss_total\": float(loss_total_sum / max(1, n_sum)),\n","        \"loss_ce\":    float(loss_ce_sum    / max(1, n_sum)),\n","        \"loss_dice\":  float(loss_dice_sum  / max(1, n_sum)),\n","\n","        # SOFT\n","        \"dice_per_class\": dice_pc.detach().cpu().numpy(),\n","        \"iou_per_class\":  iou_pc.detach().cpu().numpy(),\n","        \"dice_macro\": float(dice_m.item()),\n","        \"iou_macro\":  float(iou_m.item()),\n","\n","        # HARD\n","        \"hard_dice_per_class\": hard_dice_pc.detach().cpu().numpy(),\n","        \"hard_iou_per_class\":  hard_iou_pc.detach().cpu().numpy(),\n","        \"hard_dice_macro\": float(hard_dice_m.item()),\n","        \"hard_iou_macro\":  float(hard_iou_m.item()),\n","\n","        \"n_samples\": int(n_sum),\n","    }\n","\n","    if track_by_domain:\n","        out[\"domain_stats\"] = domain_stats\n","\n","    return out\n","\n","\n","# ------------------------------------------------------------\n","# Wrappers (best practice + backward-compatible aliases)\n","#   Cell 5 calls:\n","#     run_epoch_train(model, train_loader, optimizer, track_by_domain=True, desc=..., ce_weight=..., dice_weight=...)\n","#     run_epoch_val(model, val_loader, track_by_domain=True, desc=..., ce_weight=..., dice_weight=...)\n","# ------------------------------------------------------------\n","def run_epoch_train(\n","    model,\n","    loader,\n","    optimizer,\n","    track_by_domain: bool = True,\n","    desc: str = \"train\",\n","    tqdm_desc: str = None,\n","    device=None,\n","    num_classes: int = None,\n","    ce_weight: float = None,\n","    dice_weight: float = None,\n","    ce_w: float = None,          # alias\n","    dice_w: float = None,        # alias\n","    dice_eps: float = 1e-6,\n","    ds_weights=None,\n","    show_tqdm: bool = True,\n","    **kwargs,\n","):\n","    if tqdm_desc is None:\n","        tqdm_desc = desc\n","\n","    # accept both naming styles\n","    if ce_weight is None:\n","        ce_weight = ce_w if ce_w is not None else 1.0\n","    if dice_weight is None:\n","        dice_weight = dice_w if dice_w is not None else 1.0\n","\n","    return _run_epoch_core(\n","        model=model,\n","        loader=loader,\n","        optimizer=optimizer,\n","        train_mode=True,\n","        device=device,\n","        num_classes=num_classes,\n","        ce_weight=ce_weight,\n","        dice_weight=dice_weight,\n","        dice_eps=dice_eps,\n","        ds_weights=ds_weights,\n","        track_by_domain=track_by_domain,\n","        show_tqdm=show_tqdm,\n","        tqdm_desc=tqdm_desc,\n","        # any unused kwargs are intentionally ignored for backward-compat\n","    )\n","\n","\n","@torch.no_grad()\n","def run_epoch_val(\n","    model,\n","    loader,\n","    track_by_domain: bool = True,\n","    desc: str = \"val\",\n","    tqdm_desc: str = None,\n","    device=None,\n","    num_classes: int = None,\n","    ce_weight: float = None,\n","    dice_weight: float = None,\n","    ce_w: float = None,          # alias\n","    dice_w: float = None,        # alias\n","    dice_eps: float = 1e-6,\n","    ds_weights=None,\n","    show_tqdm: bool = True,\n","    **kwargs,\n","):\n","    if tqdm_desc is None:\n","        tqdm_desc = desc\n","\n","    if ce_weight is None:\n","        ce_weight = ce_w if ce_w is not None else 1.0\n","    if dice_weight is None:\n","        dice_weight = dice_w if dice_w is not None else 1.0\n","\n","    return _run_epoch_core(\n","        model=model,\n","        loader=loader,\n","        optimizer=None,\n","        train_mode=False,\n","        device=device,\n","        num_classes=num_classes,\n","        ce_weight=ce_weight,\n","        dice_weight=dice_weight,\n","        dice_eps=dice_eps,\n","        ds_weights=ds_weights,\n","        track_by_domain=track_by_domain,\n","        show_tqdm=show_tqdm,\n","        tqdm_desc=tqdm_desc,\n","    )\n","\n","\n","# ------------------------------------------------------------\n","# Formatting helpers\n","# ------------------------------------------------------------\n","def format_domain_metrics(domain_stats: dict, num_classes: int, prefer_hard: bool = True):\n","    \"\"\"\n","    Return per-domain mean Dice/IoU dict:\n","      { dom: {\"dice\": float, \"iou\": float, \"n\": int} }\n","    Uses HARD tp/fp/fn if available.\n","    \"\"\"\n","    out = {}\n","    if not domain_stats:\n","        return out\n","\n","    for dom in sorted(domain_stats.keys()):\n","        st = domain_stats[dom]\n","        n = int(st.get(\"n\", 0))\n","        if prefer_hard and (\"tp\" in st) and (\"fp\" in st) and (\"fn\" in st):\n","            _, dice_m, _, iou_m = _hard_metrics_from_counts(st[\"tp\"], st[\"fp\"], st[\"fn\"])\n","            out[dom] = {\"dice\": float(dice_m), \"iou\": float(iou_m), \"n\": n}\n","        else:\n","            out[dom] = {\"dice\": float(\"nan\"), \"iou\": float(\"nan\"), \"n\": n}\n","\n","    return out\n","\n","\n","def format_class_metrics(dice_per_class, iou_per_class, class_names=None):\n","    out = {}\n","    if class_names is None:\n","        class_names = globals().get(\"CLASS_NAMES\", None)\n","    if class_names is None:\n","        class_names = [str(i) for i in range(len(dice_per_class))]\n","\n","    for i, name in enumerate(class_names):\n","        out[name] = {\"dice\": float(dice_per_class[i]), \"iou\": float(iou_per_class[i])}\n","    return out\n","\n","\n","def print_domain_stats(domain_stats: dict, num_classes: int, prefer_hard: bool = True, title: str = \"Domain metrics\"):\n","    if not domain_stats:\n","        print(f\"{title}: (none)\")\n","        return\n","    dm = format_domain_metrics(domain_stats, num_classes=num_classes, prefer_hard=prefer_hard)\n","    print(f\"{title}:\")\n","    for dom, st in dm.items():\n","        print(f\"  - {dom:>12s} | n={st['n']:4d} | Dice={st['dice']:.4f} | IoU={st['iou']:.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"oISBNMiDz6-E"},"source":["\\## DEBUG FOR SAME Shuffle but different Augmentation each epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSjD-dPKa4pc"},"outputs":[],"source":["# ============================================================\n","# DEBUG CELL (v5): PROOF that all models see identical feeding\n","#   A) verifies SHUFFLE ORDER (sampler indices)\n","#   B) verifies POST-AUG CONTENT (img+mask fingerprints)\n","#\n","# Strategy:\n","#   - Build a DEBUG loader with num_workers=0 (removes worker nondet)\n","#   - Seed shuffle with base_seed (fixed; same shuffle order every epoch)\n","#   - Set dataset epoch for epoch-wise augmentation changes\n","#\n","# EXPECTED:\n","#   SAME epoch: all models match reference (order + post-aug)\n","#   DIFF epoch: post-aug fingerprints differ (augmentation schedule changes)\n","# ============================================================\n","\n","import hashlib\n","import torch\n","from torch.utils.data import DataLoader\n","from itertools import islice\n","\n","# -----------------------------\n","# Config (keep small for speed)\n","# -----------------------------\n","EPOCHS_TO_CHECK = [0, 1, 2]\n","NUM_BATCHES_TO_CHECK = 5\n","MAX_ELEMS_PER_TENSOR = 200_000   # reduce if still slow; increase if you want stronger proof\n","PRINT_FIRST_N_PER_BATCH = 3      # how many sample-fps to show per batch\n","\n","# Model tags (only used for display)\n","MODEL_TAGS = list(globals().get(\"MODEL_LIST\", [\"REF_MODEL\"]))\n","\n","assert \"GLOBAL_SEED\" in globals(), \"GLOBAL_SEED missing.\"\n","assert \"make_dataloaders\" in globals(), \"make_dataloaders(...) missing.\"\n","\n","# -----------------------------\n","# Hash helpers\n","# -----------------------------\n","def _sha16(b: bytes) -> str:\n","    return hashlib.sha256(b).hexdigest()[:16]\n","\n","def _tensor_bytes_subset(t: torch.Tensor, max_elems: int):\n","    t = t.detach().cpu()\n","    if not t.is_contiguous():\n","        t = t.contiguous()\n","    flat = t.view(-1)\n","    n = min(int(max_elems), flat.numel())\n","    flat = flat[:n]\n","    header = f\"{str(t.dtype)}|{tuple(t.shape)}|\".encode(\"utf-8\")\n","    return header + flat.numpy().tobytes()\n","\n","def _domain_bytes(domain):\n","    # domain is typically list[str] after collation\n","    if isinstance(domain, (list, tuple)):\n","        s = \"|\".join(map(str, domain))\n","    else:\n","        s = str(domain)\n","    return s.encode(\"utf-8\")\n","\n","def sample_fingerprint(img_i: torch.Tensor, mask_i: torch.Tensor, dom_i, max_elems: int):\n","    b = b\"\"\n","    b += _tensor_bytes_subset(img_i, max_elems)\n","    b += _tensor_bytes_subset(mask_i, max_elems)\n","    b += str(dom_i).encode(\"utf-8\")\n","    return _sha16(b)\n","\n","# -----------------------------\n","# Build a DEBUG loader (num_workers=0)\n","# -----------------------------\n","def build_debug_train_loader(base_seed: int, epoch: int):\n","    \"\"\"\n","    Uses your make_dataloaders() to get the dataset,\n","    but rebuilds a debug DataLoader with num_workers=0.\n","    \"\"\"\n","    tr_ds, va_ds, tr_loader, va_loader, tr_eval_by_dom, val_by_dom = make_dataloaders(base_seed=int(base_seed))\n","\n","    # epoch-aware augmentation\n","    if hasattr(tr_ds, \"set_epoch\"):\n","        tr_ds.set_epoch(int(epoch))\n","\n","    # deterministic shuffle (fixed across epochs)\n","    g = torch.Generator()\n","    g.manual_seed(int(base_seed))\n","\n","    debug_loader = DataLoader(\n","        tr_ds,\n","        batch_size=getattr(tr_loader, \"batch_size\", 1),\n","        shuffle=True,\n","        num_workers=0,           # critical: remove multi-worker nondeterminism for proof\n","        pin_memory=False,\n","        generator=g,\n","    )\n","    return tr_ds, debug_loader\n","\n","def get_first_sampler_indices(loader, n_indices: int):\n","    # RandomSampler yields indices in shuffled order\n","    sampler = loader.sampler\n","    return list(islice(iter(sampler), int(n_indices)))\n","\n","def get_loader_fps(loader, num_batches: int, max_elems: int):\n","    \"\"\"\n","    Returns per-batch list of per-sample fingerprints (stronger than batch-level hash).\n","    \"\"\"\n","    out = []\n","    it = iter(loader)\n","    for _ in range(int(num_batches)):\n","        try:\n","            img, mask, domain = next(it)\n","        except StopIteration:\n","            break\n","\n","        # img: [B,C,H,W], mask: [B,H,W], domain: list[str]\n","        B = img.shape[0]\n","        fps = []\n","        for i in range(B):\n","            dom_i = domain[i] if isinstance(domain, (list, tuple)) else domain\n","            fps.append(sample_fingerprint(img[i], mask[i], dom_i, max_elems=max_elems))\n","        out.append(fps)\n","    return out\n","\n","# -----------------------------\n","# Main check\n","# -----------------------------\n","def check_feed_identity_across_models(model_tags, epochs, num_batches, base_seed, max_elems):\n","    print(\"\\n================ DEBUG (v5): FEED IDENTITY PROOF ================\")\n","    print(f\"GLOBAL_SEED={int(base_seed)} | epochs={list(epochs)} | batches={int(num_batches)}\")\n","    print(\"We verify TWO things per epoch:\")\n","    print(\"  (1) sampler indices (shuffle order)\")\n","    print(\"  (2) per-sample post-augmentation fingerprints (img+mask+domain)\")\n","    print(\"NOTE: debug loader uses num_workers=0 for strict determinism proof.\\n\")\n","\n","    ref = {}  # epoch -> {\"idx\":..., \"fps\":...}\n","\n","    for ep in epochs:\n","        # Reference\n","        _, ref_loader = build_debug_train_loader(base_seed, ep)\n","        ref_idx = get_first_sampler_indices(ref_loader, n_indices=ref_loader.batch_size * num_batches)\n","        ref_fps = get_loader_fps(ref_loader, num_batches=num_batches, max_elems=max_elems)\n","        ref[ep] = {\"idx\": ref_idx, \"fps\": ref_fps}\n","\n","        # Compact signature per epoch for quick compare\n","        sig = _sha16((\"|\".join(map(str, ref_idx)) + \"||\" + \"|\".join(sum(ref_fps, []))).encode(\"utf-8\"))\n","        print(f\"Epoch {ep}: REF sig={sig} | first_idx[:8]={ref_idx[:8]}\")\n","\n","        # Compare ‚Äúother models‚Äù (rebuild loaders exactly the same way)\n","        for mtag in model_tags:\n","            _, ld = build_debug_train_loader(base_seed, ep)\n","            idx = get_first_sampler_indices(ld, n_indices=ld.batch_size * num_batches)\n","            fps = get_loader_fps(ld, num_batches=num_batches, max_elems=max_elems)\n","\n","            ok_order = (idx == ref_idx)\n","            ok_fps   = (fps == ref_fps)\n","\n","            status = \"OK\" if (ok_order and ok_fps) else \"MISMATCH\"\n","            if status == \"MISMATCH\":\n","                # pinpoint where it diverges\n","                bad_i = next((i for i in range(min(len(idx), len(ref_idx))) if idx[i] != ref_idx[i]), None)\n","                bad_b = next((b for b in range(min(len(fps), len(ref_fps))) if fps[b] != ref_fps[b]), None)\n","                print(f\"  - {mtag}: {status} | order={ok_order} (first diff idx pos={bad_i}) | aug={ok_fps} (first diff batch={bad_b})\")\n","                if bad_b is not None:\n","                    print(f\"    ref batch{bad_b} samplefps (first {PRINT_FIRST_N_PER_BATCH}): {ref_fps[bad_b][:PRINT_FIRST_N_PER_BATCH]}\")\n","                    print(f\"    got batch{bad_b} samplefps (first {PRINT_FIRST_N_PER_BATCH}): {fps[bad_b][:PRINT_FIRST_N_PER_BATCH]}\")\n","            else:\n","                print(f\"  - {mtag}: {status}\")\n","\n","        # Across-epoch check (augmentation must change)\n","        if ep != epochs[0]:\n","            prev = epochs[epochs.index(ep) - 1]\n","            same_aug = (ref[ep][\"fps\"] == ref[prev][\"fps\"])\n","            if same_aug:\n","                print(f\"  ‚ö†Ô∏è  WARNING: REF post-aug fingerprints identical to epoch {prev}.\")\n","                print(\"     That means your augmentation is not epoch-dependent in practice.\")\n","                print(\"     In v5 it SHOULD be, via seed = base_seed + epoch*BIG + idx.\")\n","            else:\n","                print(f\"  ‚úÖ REF post-aug differs vs epoch {prev} (expected).\")\n","\n","        print(\"\")\n","\n","    print(\"=================================================================\\n\")\n","    print(\"If you see MISMATCH with num_workers=0, it is a REAL bug in your seed/epoch logic.\")\n","    print(\"If it matches with num_workers=0 but mismatches in training (num_workers>0),\")\n","    print(\"then the leak is multi-worker / external-library nondeterminism.\")\n","    print(\"In that case, keep num_workers>0 for speed, but rely on THIS proof for paper claims.\\n\")\n","\n","# Run\n","check_feed_identity_across_models(\n","    model_tags=MODEL_TAGS,\n","    epochs=EPOCHS_TO_CHECK,\n","    num_batches=NUM_BATCHES_TO_CHECK,\n","    base_seed=int(GLOBAL_SEED),\n","    max_elems=int(MAX_ELEMS_PER_TENSOR),\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xrh3WNv73ktZ"},"outputs":[],"source":["# ============================================================\n","# CELL 5: Training loop with TRUE resume + fixed deterministic shuffle\n","# FIXES (no architecture changes):\n","#   - Domain gaps fully removed from printing/logging (no compute_domain_gaps usage)\n","#   - Validation reporting + XL saving uses HARD Dice/IoU only (soft removed for val)\n","#   - Resume training restores optimizer + scaler (+ RNG states)\n","#   - Shuffle stays ON, deterministic but FIXED across epochs (seed = GLOBAL_SEED)\n","#   - Augmentation/crop changes every epoch (fallback enforced if dataset isn't epoch-aware yet)\n","#\n","# Requires:\n","#   - Cell 2: make_dataloaders(...)\n","#   - Cell 3: get_model_paths, append_log_row\n","#   - Cell 4: UNet, UNet3Plus, create_sslite_v1/v2/v3\n","#   - Cell 4.5: run_epoch_train, run_epoch_val, _metric_from_sums\n","# ============================================================\n","\n","import time\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import hashlib\n","\n","\n","# ---------- Safety checks ----------\n","assert \"make_dataloaders\" in globals(), \"make_dataloaders not found. Run Cell 2 first.\"\n","assert \"get_model_paths\" in globals() and \"append_log_row\" in globals(), \\\n","    \"get_model_paths / append_log_row not found. Run Cell 3 first.\"\n","assert \"run_epoch_train\" in globals() and \"run_epoch_val\" in globals(), \\\n","    \"run_epoch_train / run_epoch_val not found. Run Cell 4.5 first.\"\n","assert \"_hard_metrics_from_counts\" in globals(), \"_hard_metrics_from_counts not found. Run Cell 4.5B first.\"\n","\n","# Lite models (V1‚ÄìV3 only)\n","assert \"create_sslite_v1\" in globals() and \"create_sslite_v2\" in globals() and \"create_sslite_v3\" in globals(), \\\n","    \"Lite wrappers (create_sslite_v1..v3) not found. Run Cell 4 first.\"\n","\n","# ---------- Global config for this cell ----------\n","NUM_CLASSES = 3\n","DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# AMP flag (Cell 4.5 reads USE_AMP + SCALER)\n","USE_AMP = False\n","\n","# Learning-rate schedule + early stopping (shared for all models)\n","INIT_LR             = 1e-3\n","LR_DECAY_FACTOR     = 10.0\n","LR_PATIENCE_EPOCHS  = 5\n","EARLY_STOP_PATIENCE = 20\n","MIN_LR              = 1e-5\n","\n","print(f\"üñ•  Using device: {DEVICE}\")\n","print(f\"üìâ Initial learning rate: {INIT_LR}\")\n","print(f\"üìâ LR decay: √∑{LR_DECAY_FACTOR} after {LR_PATIENCE_EPOCHS} epochs without improvement\")\n","print(f\"üìâ Minimum learning rate: {MIN_LR}\")\n","print(f\"üõë Early stopping: stop after {EARLY_STOP_PATIENCE} epochs without improvement\")\n","print(f\"‚öôÔ∏è  AMP enabled? {USE_AMP}\")\n","\n","\n","# ---------- Model creation helper ----------\n","def create_model(model_key: str, num_classes: int = None) -> nn.Module:\n","    if num_classes is None:\n","        num_classes = NUM_CLASSES\n","\n","    key = str(model_key).lower()\n","\n","    ds_u3p  = bool(globals().get(\"UNET3PLUS_DEEP_SUPERVISION\", False))\n","    in_ch   = globals().get(\"IN_CHANNELS\", 1)\n","    base_ch = globals().get(\"BASE_CH\", 32)\n","\n","    if key in {\"unet\", \"u-net\"}:\n","        return UNet(in_channels=in_ch, num_classes=num_classes, base_ch=base_ch)\n","\n","    if key in {\"unet3plus\", \"unet3+\", \"u-net3plus\", \"u-net3+\"}:\n","        return UNet3Plus(\n","            in_channels=in_ch,\n","            num_classes=num_classes,\n","            base_ch=base_ch,\n","            deep_supervision=ds_u3p,\n","        )\n","\n","    if key in {\"sslite_v1\", \"ssunet3pluslite_v1\", \"ssunet3pluslite-v1\"}:\n","        return create_sslite_v1(num_classes=num_classes)\n","\n","    if key in {\"sslite_v2\", \"ssunet3pluslite_v2\", \"ssunet3pluslite-v2\"}:\n","        return create_sslite_v2(num_classes=num_classes)\n","\n","    if key in {\"sslite_v3\", \"ssunet3pluslite_v3\", \"ssunet3pluslite-v3\"}:\n","        return create_sslite_v3(num_classes=num_classes)\n","\n","    raise ValueError(f\"Unknown model_key: {model_key}\")\n","\n","\n","# ---------- Small formatting helpers (for logging strings) ----------\n","def _fmt_domain_strings(domain_stats: dict, num_classes: int):\n","    if not domain_stats:\n","        return \"\", \"\"\n","\n","    parts_d, parts_i = [], []\n","    for dom in [\"CFRP\", \"FFRP_Autoclave\", \"FFRP_Oven\"]:\n","        if dom not in domain_stats:\n","            continue\n","        st = domain_stats[dom]\n","        tp = st[\"tp\"]\n","        fp = st[\"fp\"]\n","        fn = st[\"fn\"]\n","        dice_pc, dice_m, iou_pc, iou_m = _hard_metrics_from_counts(tp, fp, fn)\n","        d = float(dice_m)\n","        i = float(iou_m)\n","        parts_d.append(f\"{dom}:{d:.5f}\")\n","        parts_i.append(f\"{dom}:{i:.5f}\")\n","\n","    return \"|\".join(parts_d), \"|\".join(parts_i)\n","\n","\n","def _fmt_class_strings(dice_pc, iou_pc, class_names=None):\n","    if class_names is None:\n","        class_names = globals().get(\"CLASS_NAMES\", None)\n","    if class_names is None:\n","        class_names = [f\"class_{i}\" for i in range(len(dice_pc))]\n","\n","    parts_d, parts_i = [], []\n","    for i, nm in enumerate(class_names):\n","        parts_d.append(f\"{nm}:{float(dice_pc[i]):.5f}\")\n","        parts_i.append(f\"{nm}:{float(iou_pc[i]):.5f}\")\n","    return \"|\".join(parts_d), \"|\".join(parts_i)\n","\n","\n","def _print_epoch_table(train_row=None, val_row=None):\n","    print(\"\\n[PER-EPOCH SUMMARY]\")\n","    print(f\"{'phase':<5}  {'epoch':>3}  {'loss':>9}  {'cls':>9}  {'dice_loss':>9}  {'HDice':>9}  {'HIoU':>9}  {'time[s]':>8}\")\n","    print(\"-\" * 84)\n","\n","    if train_row is not None:\n","        print(f\"{'train':<5}  {int(train_row['epoch']):3d}  {train_row['loss_total']:9.4f}  {train_row['loss_ce']:9.4f}  {train_row['loss_dice']:9.4f}  \"\n","              f\"{float(train_row['hard_dice_macro']):9.4f}  {float(train_row['hard_iou_macro']):9.4f}  {train_row['time_sec']:8.1f}\")\n","\n","    if val_row is not None:\n","        print(f\"{'val':<5}  {int(val_row['epoch']):3d}  {val_row['loss_total']:9.4f}  {val_row['loss_ce']:9.4f}  {val_row['loss_dice']:9.4f}  \"\n","              f\"{float(val_row['hard_dice_macro']):9.4f}  {float(val_row['hard_iou_macro']):9.4f}  {val_row['time_sec']:8.1f}\")\n","\n","\n","# ---------- RNG state save/load helpers (true resume) ----------\n","def _get_rng_state_payload():\n","    payload = {\n","        \"py_random_state\": random.getstate(),\n","        \"np_random_state\": np.random.get_state(),\n","        \"torch_rng_state\": torch.get_rng_state(),\n","    }\n","    if torch.cuda.is_available():\n","        try:\n","            payload[\"torch_cuda_rng_state_all\"] = torch.cuda.get_rng_state_all()\n","        except Exception:\n","            payload[\"torch_cuda_rng_state_all\"] = None\n","    else:\n","        payload[\"torch_cuda_rng_state_all\"] = None\n","    return payload\n","\n","def _restore_rng_state_payload(payload: dict):\n","    try:\n","        if payload.get(\"py_random_state\", None) is not None:\n","            random.setstate(payload[\"py_random_state\"])\n","\n","        if payload.get(\"np_random_state\", None) is not None:\n","            np.random.set_state(payload[\"np_random_state\"])\n","\n","        # ---- torch CPU RNG (MUST be CPU ByteTensor) ----\n","        st = payload.get(\"torch_rng_state\", None)\n","        if st is not None:\n","            if torch.is_tensor(st):\n","                st = st.detach().to(device=\"cpu\", dtype=torch.uint8)\n","            else:\n","                st = torch.tensor(st, dtype=torch.uint8)\n","            torch.set_rng_state(st)\n","\n","        # ---- torch CUDA RNG (list of CPU ByteTensors) ----\n","        if torch.cuda.is_available():\n","            st_all = payload.get(\"torch_cuda_rng_state_all\", None)\n","            if st_all is not None:\n","                # Sometimes old ckpt may store single tensor; normalize to list\n","                if torch.is_tensor(st_all):\n","                    st_all = [st_all]\n","\n","                fixed = []\n","                for s in st_all:\n","                    if s is None:\n","                        fixed = None\n","                        break\n","                    if torch.is_tensor(s):\n","                        s = s.detach().to(device=\"cpu\", dtype=torch.uint8)\n","                    else:\n","                        s = torch.tensor(s, dtype=torch.uint8)\n","                    fixed.append(s)\n","\n","                if fixed is not None:\n","                    torch.cuda.set_rng_state_all(fixed)\n","\n","    except Exception as e:\n","        print(f\"   ‚ö†Ô∏è  Resume: RNG state restore failed: {e}\")\n","\n","\n","\n","# ---------- Main training function ----------\n","def run_training_for_model(model_key: str, extra_epochs: int = 10):\n","    pretty_name = {\n","        \"unet\":      \"UNet\",\n","        \"unet3plus\": \"UNet3Plus\",\n","        \"unet3+\":    \"UNet3Plus\",\n","        \"sslite_v1\": \"SS-UNet3+ Lite V1 (EnhSemiFull)\",\n","        \"sslite_v2\": \"SS-UNet3+ Lite V2 (4-Branch)\",\n","        \"sslite_v3\": \"SS-UNet3+ Lite V3 (Baseline)\",\n","    }.get(str(model_key).lower(), model_key)\n","\n","    # Reset RNGs before each model training (keeps cross-model comparability)\n","    if \"reset_all_seeds\" in globals():\n","        reset_all_seeds(int(GLOBAL_SEED))\n","    else:\n","        random.seed(int(GLOBAL_SEED))\n","        np.random.seed(int(GLOBAL_SEED))\n","        torch.manual_seed(int(GLOBAL_SEED))\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed_all(int(GLOBAL_SEED))\n","\n","    # Meta toggles (from Cell 4.5 switchboard)\n","    ce_mode   = str(globals().get(\"CE_MODE\", \"ce\")).lower().strip()\n","    abl_fus   = str(globals().get(\"ABL_FUSION\", \"native\")).lower().strip()\n","    use_bn    = bool(globals().get(\"USE_BN\", True))\n","    use_gn    = bool(globals().get(\"USE_GN\", False))\n","    base_ch   = int(globals().get(\"BASE_CH\", 32))\n","    mk = str(model_key).lower()\n","    if mk in {\"unet3plus\", \"unet3+\", \"u-net3plus\", \"u-net3+\"}:\n","        ds_on = bool(globals().get(\"UNET3PLUS_DEEP_SUPERVISION\", False))\n","    elif mk in {\"sslite_v1\", \"ssunet3pluslite_v1\", \"ssunet3pluslite-v1\",\n","                \"sslite_v2\", \"ssunet3pluslite_v2\", \"ssunet3pluslite-v2\",\n","                \"sslite_v3\", \"ssunet3pluslite_v3\", \"ssunet3pluslite-v3\"}:\n","        ds_on = bool(globals().get(\"SSLITE_DEEP_SUPERVISION\", True))\n","    else:\n","        ds_on = False\n","\n","\n","    ce_w_cfg   = float(globals().get(\"CE_WEIGHT\", 0.5))\n","    dice_w_cfg = float(globals().get(\"DICE_WEIGHT\", 0.5))\n","\n","    print(\"\\n\" + \"=\" * 72)\n","    print(f\"üöÄ Starting (or resuming) training: {pretty_name}  (key='{model_key}')\")\n","    print(f\"   Config ‚Üí CE_MODE='{ce_mode}', ABL_FUSION='{abl_fus}', USE_BN={use_bn}, USE_GN={use_gn}, BASE_CH={base_ch}, DS_ON={ds_on}\")\n","    print(\"=\" * 72)\n","\n","    # --- rebuild loaders per model (keeps SAME epoch schedule across models) ---\n","    global train_dataset, val_dataset, train_loader, val_loader, TRAIN_EVAL_LOADERS_BY_DOMAIN, VAL_LOADERS_BY_DOMAIN, train_generator\n","    train_dataset, val_dataset, train_loader, val_loader, TRAIN_EVAL_LOADERS_BY_DOMAIN, VAL_LOADERS_BY_DOMAIN = make_dataloaders(\n","        base_seed=int(GLOBAL_SEED)\n","    )\n","    train_generator = getattr(getattr(train_loader, \"sampler\", None), \"generator\", None)\n","\n","    model = create_model(model_key, num_classes=NUM_CLASSES).to(DEVICE)\n","    params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"   Trainable parameters: {params_count/1e6:.4f} M\")\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=INIT_LR)\n","\n","    # AMP objects used by Cell 4.5\n","    global SCALER\n","    SCALER = torch.cuda.amp.GradScaler(enabled=USE_AMP and (DEVICE.type == \"cuda\"))\n","\n","    paths = get_model_paths(model_key)\n","    print(\"   Model/Log paths:\")\n","    for k, v in paths.items():\n","        print(f\"   {k}: {v}\")\n","\n","    best_val_hard_dice_macro = -1.0\n","    best_epoch               = 0\n","    current_lr               = INIT_LR\n","    start_epoch              = 0\n","    no_improve_streak        = 0\n","    no_improve_for_lr        = 0\n","\n","    last_ckpt = paths[\"last_ckpt\"]\n","    best_ckpt = paths[\"best_ckpt\"]\n","\n","    # ---------- Detect whether dataset augmentation changes with epoch ----------\n","    # If not, we enforce epoch-wise augmentation by changing train_dataset.base_seed per epoch.\n","    AUG_BIG = 1_000_000\n","    force_epoch_aug = False\n","    try:\n","        # probe idx=0 twice with epoch 0 and 1\n","        orig_epoch = getattr(train_dataset, \"epoch\", 0)\n","        orig_base_seed = getattr(train_dataset, \"base_seed\", int(GLOBAL_SEED))\n","\n","        if hasattr(train_dataset, \"set_epoch\"):\n","            train_dataset.set_epoch(0)\n","        x0a = train_dataset[0]  # (img, mask, dom)\n","        if hasattr(train_dataset, \"set_epoch\"):\n","            train_dataset.set_epoch(1)\n","        x0b = train_dataset[0]\n","\n","        def _sig(sample):\n","            img, msk = sample[0], sample[1]\n","            h = hashlib.sha256()\n","            h.update(img.detach().cpu().numpy().tobytes())\n","            h.update(msk.detach().cpu().numpy().tobytes())\n","            return h.hexdigest()\n","\n","        same = (_sig(x0a) == _sig(x0b))\n","\n","        # restore\n","        if hasattr(train_dataset, \"set_epoch\"):\n","            train_dataset.set_epoch(int(orig_epoch))\n","        try:\n","            train_dataset.base_seed = int(orig_base_seed)\n","        except Exception:\n","            pass\n","\n","        if same:\n","            force_epoch_aug = True\n","            print(\"   ‚ö†Ô∏è  Dataset aug seems NOT epoch-aware yet ‚Üí enabling safe fallback: base_seed += epoch*BIG\")\n","        else:\n","            print(\"   ‚úÖ Dataset aug is epoch-aware ‚Üí no fallback needed\")\n","    except Exception:\n","        # if probe fails, don't force anything\n","        force_epoch_aug = False\n","\n","    # ---------- Resume logic (TRUE resume: model + optimizer + scaler + RNG) ----------\n","    if last_ckpt.exists():\n","        ckpt = torch.load(last_ckpt, map_location=DEVICE, weights_only=False)\n","        state_dict = ckpt.get(\"model_state\", ckpt.get(\"state_dict\", None))\n","        if state_dict is not None:\n","            model.load_state_dict(state_dict)\n","\n","        # optimizer/scaler restore\n","\n","        if \"optimizer_state\" in ckpt and ckpt[\"optimizer_state\"] is not None:\n","            try:\n","                optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n","            except Exception as e:\n","                print(f\"   ‚ö†Ô∏è  Resume: optimizer_state load failed: {e}\")\n","\n","        if \"scaler_state\" in ckpt and ckpt[\"scaler_state\"] is not None and SCALER is not None:\n","            try:\n","                SCALER.load_state_dict(ckpt[\"scaler_state\"])\n","            except Exception as e:\n","                print(f\"   ‚ö†Ô∏è  Resume: scaler_state load failed: {e}\")\n","\n","        # RNG restore (best effort)\n","        if \"rng_state\" in ckpt and isinstance(ckpt[\"rng_state\"], dict):\n","\n","            # ---- BEFORE restore: check what's inside ckpt ----\n","            try:\n","                st = ckpt[\"rng_state\"].get(\"torch_rng_state\", None)\n","                print(\"   [RNG-BEFORE] ckpt torch_rng_state:\",\n","                      type(st),\n","                      getattr(st, \"dtype\", None),\n","                      getattr(st, \"device\", None))\n","\n","                st_all = ckpt[\"rng_state\"].get(\"torch_cuda_rng_state_all\", None)\n","                if st_all is None:\n","                    print(\"   [RNG-BEFORE] ckpt cuda_rng_state_all: None\")\n","                else:\n","                    # can be list or tensor\n","                    if torch.is_tensor(st_all):\n","                        print(\"   [RNG-BEFORE] ckpt cuda_rng_state_all:\",\n","                              type(st_all), st_all.dtype, st_all.device)\n","                    else:\n","                        print(\"   [RNG-BEFORE] ckpt cuda_rng_state_all: list_len=\", len(st_all))\n","                        if len(st_all) > 0 and st_all[0] is not None:\n","                            print(\"   [RNG-BEFORE] ckpt cuda_rng_state_all[0]:\",\n","                                  type(st_all[0]),\n","                                  getattr(st_all[0], \"dtype\", None),\n","                                  getattr(st_all[0], \"device\", None))\n","            except Exception as e:\n","                print(\"   [RNG-BEFORE] check failed:\", e)\n","\n","            _restore_rng_state_payload(ckpt[\"rng_state\"])\n","\n","            # ---- AFTER restore: confirm current RNG states ----\n","            try:\n","                cpu_st = torch.get_rng_state()\n","                print(\"   [RNG-AFTER] torch.get_rng_state():\", cpu_st.dtype, cpu_st.device)\n","                if torch.cuda.is_available():\n","                    cuda0 = torch.cuda.get_rng_state_all()[0]\n","                    print(\"   [RNG-AFTER] torch.cuda.get_rng_state_all()[0]:\", cuda0.dtype, cuda0.device)\n","            except Exception as e:\n","                print(\"   [RNG-AFTER] check failed:\", e)\n","\n","\n","        prev_epoch               = int(ckpt.get(\"epoch\", 0))\n","        best_val_hard_dice_macro = float(ckpt.get(\"best_val_hard_dice_macro\", -1.0))\n","        best_epoch               = int(ckpt.get(\"best_epoch\", prev_epoch))\n","        current_lr               = float(ckpt.get(\"current_lr\", INIT_LR))\n","        no_improve_streak        = int(ckpt.get(\"no_improve_streak\", 0))\n","        no_improve_for_lr        = int(ckpt.get(\"no_improve_for_lr\", 0))\n","        start_epoch              = prev_epoch + 1\n","\n","        # ensure optimizer lr matches current_lr\n","        for g in optimizer.param_groups:\n","            g[\"lr\"] = float(current_lr)\n","\n","        print(f\"   üîÅ Resume from LAST: epoch {start_epoch} (bestHardDice={best_val_hard_dice_macro:.4f} @epoch {best_epoch}, LR={current_lr:.6f})\")\n","\n","    elif best_ckpt.exists():\n","        ckpt = torch.load(best_ckpt, map_location=DEVICE, weights_only=False)\n","        state_dict = ckpt.get(\"model_state\", ckpt.get(\"state_dict\", None))\n","        if state_dict is not None:\n","            model.load_state_dict(state_dict)\n","\n","        best_val_hard_dice_macro = float(ckpt.get(\"best_val_hard_dice_macro\", -1.0))\n","        best_epoch               = int(ckpt.get(\"best_epoch\", ckpt.get(\"epoch\", 0)))\n","        start_epoch              = best_epoch + 1\n","        print(f\"   üîÅ Warm-start from BEST: next epoch {start_epoch} (bestHardDice={best_val_hard_dice_macro:.4f} @epoch {best_epoch})\")\n","\n","    else:\n","        print(\"   üî∞ No checkpoint found ‚Üí epoch 0 baseline validation...\")\n","\n","        with torch.no_grad():\n","            val_stats0 = run_epoch_val(\n","                model, val_loader,\n","                track_by_domain=True,\n","                desc=\"Val (epoch 0)\",\n","                ce_weight=ce_w_cfg,\n","                dice_weight=dice_w_cfg,\n","            )\n","\n","        # HARD (this is the only val metric we report/save)\n","        val_hard_dice_macro0 = float(val_stats0[\"hard_dice_macro\"])\n","        val_hard_iou_macro0  = float(val_stats0[\"hard_iou_macro\"])\n","\n","\n","        val_hard_dice_pc0 = val_stats0.get(\"hard_dice_per_class\", None)\n","        val_hard_iou_pc0  = val_stats0.get(\"hard_iou_per_class\",  None)\n","\n","        # losses\n","        val_loss0      = float(val_stats0[\"loss_total\"])\n","        val_ce0        = float(val_stats0[\"loss_ce\"])\n","        val_dice_loss0 = float(val_stats0[\"loss_dice\"])\n","        val_time0      = float(val_stats0[\"time_sec\"])\n","\n","        val_dice_pc0 = val_stats0[\"dice_per_class\"]\n","        val_iou_pc0  = val_stats0[\"iou_per_class\"]\n","        val_dom0     = val_stats0.get(\"domain_stats\", None)\n","\n","        # print (VAL shows HARD only)\n","        _print_epoch_table(train_row=None, val_row={\n","            \"epoch\": 0,\n","            \"loss_total\": val_loss0, \"loss_ce\": val_ce0, \"loss_dice\": val_dice_loss0,\n","            \"hard_dice_macro\": val_hard_dice_macro0, \"hard_iou_macro\": val_hard_iou_macro0,\n","            \"time_sec\": val_time0\n","        })\n","\n","\n","        # strings for Table B\n","        val_dice_domain_str0, val_iou_domain_str0 = _fmt_domain_strings(val_dom0, num_classes=NUM_CLASSES) if val_dom0 else (\"\", \"\")\n","        val_dice_class_str0,  val_iou_class_str0  = _fmt_class_strings(val_dice_pc0, val_iou_pc0, class_names=globals().get(\"CLASS_NAMES\", None))\n","\n","        # Log epoch 0 (VAL only) ‚Äî SOFT FIELDS BLANK, HARD FILLED\n","        row_val0 = {\n","            \"phase\": \"val\",\n","            \"eval_domain\": \"ALL\",\n","            \"epoch\": 0,\n","            \"model\": model_key,\n","            \"lr\": float(INIT_LR),\n","            \"time_sec\": val_time0,\n","\n","            \"loss_total\": val_loss0,\n","            \"loss_ce\": val_ce0,\n","            \"loss_dice\": val_dice_loss0,\n","\n","            \"dice_domain\": val_dice_domain_str0,\n","            \"iou_domain\":  val_iou_domain_str0,\n","            \"dice_classes\": val_dice_class_str0,\n","            \"iou_classes\":  val_iou_class_str0,\n","\n","            # HARD columns\n","            \"hard_dice_macro\": float(val_hard_dice_macro0),\n","            \"hard_iou_macro\":  float(val_hard_iou_macro0),\n","\n","            # Meta\n","            \"ce_mode\": ce_mode,\n","            \"abl_fusion\": abl_fus,\n","            \"use_gn\": use_gn,\n","            \"base_ch\": base_ch,\n","            \"ds_on\": ds_on,\n","            \"use_bn\": use_bn,\n","\n","        }\n","\n","        # per-class hard columns\n","        for ci in range(NUM_CLASSES):\n","            row_val0[f\"hard_dice_c{ci}\"] = float(val_hard_dice_pc0[ci]) if val_hard_dice_pc0 is not None else \"\"\n","            row_val0[f\"hard_iou_c{ci}\"]  = float(val_hard_iou_pc0[ci])  if val_hard_iou_pc0  is not None else \"\"\n","\n","        append_log_row(model_key, row_val0)\n","\n","        # initialize best based on HARD\n","        best_val_hard_dice_macro = float(val_hard_dice_macro0)\n","        best_epoch               = 0\n","\n","        ckpt_payload0 = {\n","            \"epoch\": 0,\n","            \"model_state\": model.state_dict(),\n","            \"optimizer_state\": optimizer.state_dict(),\n","            \"scaler_state\": SCALER.state_dict() if SCALER is not None else None,\n","            \"rng_state\": _get_rng_state_payload(),\n","\n","            \"best_val_hard_dice_macro\": float(best_val_hard_dice_macro),\n","            \"best_epoch\": int(best_epoch),\n","            \"current_lr\": float(INIT_LR),\n","            \"no_improve_streak\": int(no_improve_streak),\n","            \"no_improve_for_lr\": int(no_improve_for_lr),\n","\n","            # meta snapshot\n","            \"ce_mode\": ce_mode,\n","            \"abl_fusion\": abl_fus,\n","            \"use_bn\": use_bn,\n","            \"use_gn\": use_gn,\n","            \"base_ch\": base_ch,\n","            \"ds_on\": ds_on,\n","\n","        }\n","        torch.save(ckpt_payload0, best_ckpt)\n","        torch.save(ckpt_payload0, last_ckpt)\n","        print(f\"   Saved epoch-0 BEST to: {best_ckpt}\")\n","        print(f\"   Saved epoch-0 LAST to: {last_ckpt}\")\n","\n","        start_epoch = 1\n","        current_lr  = float(INIT_LR)\n","\n","    # ---------- Epoch loop ----------\n","    max_epoch = start_epoch + int(extra_epochs)\n","    print(f\"\\n   Will run epochs {start_epoch}..{max_epoch-1} (extra {extra_epochs} epochs)\")\n","\n","    def _set_epoch_schedule(epoch: int):\n","        # dataset epoch hook\n","        try:\n","            if hasattr(train_dataset, \"set_epoch\"):\n","                train_dataset.set_epoch(int(epoch))\n","        except Exception:\n","            pass\n","\n","    # deterministic shuffle (fixed across epochs)\n","        try:\n","            if train_generator is not None:\n","                train_generator.manual_seed(int(GLOBAL_SEED))\n","        except Exception:\n","            pass\n","\n","        # fallback epoch-wise augmentation if dataset isn't epoch-aware yet\n","        if force_epoch_aug:\n","            try:\n","                train_dataset.base_seed = int(GLOBAL_SEED) + int(epoch) * int(AUG_BIG)\n","            except Exception:\n","                pass\n","\n","    for epoch in range(start_epoch, max_epoch):\n","        print(f\"\\nüìò Epoch {epoch:03d}  (LR={current_lr:.6f})\")\n","\n","        _set_epoch_schedule(epoch)\n","\n","        # ---- TRAIN ----\n","        train_stats = run_epoch_train(\n","            model, train_loader, optimizer,\n","            track_by_domain=True,\n","            desc=f\"Train {epoch:03d}\",\n","            ce_weight=ce_w_cfg,\n","            dice_weight=dice_w_cfg,\n","        )\n","\n","        train_row = {\n","            \"phase\": \"train\",\n","            \"eval_domain\": \"ALL\",\n","            \"epoch\": int(epoch),\n","            \"model\": model_key,\n","            \"lr\": float(current_lr),\n","            \"time_sec\": float(train_stats[\"time_sec\"]),\n","\n","            \"loss_total\": float(train_stats[\"loss_total\"]),\n","            \"loss_ce\":    float(train_stats[\"loss_ce\"]),\n","            \"loss_dice\":  float(train_stats[\"loss_dice\"]),\n","\n","            # TRAIN keeps SOFT exact metrics\n","            \"dice_domain\": \"\",\n","            \"iou_domain\":  \"\",\n","            \"dice_classes\": \"\",\n","            \"iou_classes\":  \"\",\n","\n","            # HARD metrics only (train also logs hard)\n","            \"hard_dice_macro\": float(train_stats[\"hard_dice_macro\"]),\n","            \"hard_iou_macro\":  float(train_stats[\"hard_iou_macro\"]),\n","\n","            \"ce_mode\": ce_mode,\n","            \"abl_fusion\": abl_fus,\n","            \"use_bn\": use_bn,\n","            \"use_gn\": use_gn,\n","            \"base_ch\": base_ch,\n","            \"ds_on\": ds_on,\n","\n","        }\n","        for ci in range(NUM_CLASSES):\n","            train_row[f\"hard_dice_c{ci}\"] = float(train_stats[\"hard_dice_per_class\"][ci])\n","            train_row[f\"hard_iou_c{ci}\"]  = float(train_stats[\"hard_iou_per_class\"][ci])\n","\n","        train_dom = train_stats.get(\"domain_stats\", None)\n","        if train_dom:\n","            d_dom_str, i_dom_str = _fmt_domain_strings(train_dom, num_classes=NUM_CLASSES)\n","            train_row[\"dice_domain\"] = d_dom_str\n","            train_row[\"iou_domain\"]  = i_dom_str\n","\n","        d_cls_str, i_cls_str = _fmt_class_strings(\n","            train_stats[\"dice_per_class\"], train_stats[\"iou_per_class\"],\n","            class_names=globals().get(\"CLASS_NAMES\", None)\n","        )\n","        train_row[\"dice_classes\"] = d_cls_str\n","        train_row[\"iou_classes\"]  = i_cls_str\n","\n","        # ---- VAL ----\n","        with torch.no_grad():\n","            val_stats = run_epoch_val(\n","                model, val_loader,\n","                track_by_domain=True,\n","                desc=f\"Val {epoch:03d}\",\n","                ce_weight=ce_w_cfg,\n","                dice_weight=dice_w_cfg,\n","            )\n","\n","\n","\n","\n","        val_hard_dice_macro = float(val_stats[\"hard_dice_macro\"])\n","        val_hard_iou_macro  = float(val_stats[\"hard_iou_macro\"])\n","\n","        val_hard_dice_pc    = val_stats.get(\"hard_dice_per_class\", None)\n","        val_hard_iou_pc     = val_stats.get(\"hard_iou_per_class\",  None)\n","\n","        val_row = {\n","            \"phase\": \"val\",\n","            \"eval_domain\": \"ALL\",\n","            \"epoch\": int(epoch),\n","            \"model\": model_key,\n","            \"lr\": float(current_lr),\n","            \"time_sec\": float(val_stats[\"time_sec\"]),\n","\n","            \"loss_total\": float(val_stats[\"loss_total\"]),\n","            \"loss_ce\":    float(val_stats[\"loss_ce\"]),\n","            \"loss_dice\":  float(val_stats[\"loss_dice\"]),\n","\n","            \"dice_domain\": \"\",\n","            \"iou_domain\":  \"\",\n","            \"dice_classes\": \"\",\n","            \"iou_classes\":  \"\",\n","\n","            # HARD columns\n","            \"hard_dice_macro\": float(val_hard_dice_macro),\n","            \"hard_iou_macro\":  float(val_hard_iou_macro),\n","\n","            \"ce_mode\": ce_mode,\n","            \"abl_fusion\": abl_fus,\n","            \"use_bn\": use_bn,\n","            \"use_gn\": use_gn,\n","            \"base_ch\": base_ch,\n","            \"ds_on\": ds_on,\n","        }\n","\n","\n","        for ci in range(NUM_CLASSES):\n","            val_row[f\"hard_dice_c{ci}\"] = float(val_hard_dice_pc[ci]) if val_hard_dice_pc is not None else \"\"\n","            val_row[f\"hard_iou_c{ci}\"]  = float(val_hard_iou_pc[ci])  if val_hard_iou_pc  is not None else \"\"\n","\n","        val_dom = val_stats.get(\"domain_stats\", None)\n","        if val_dom:\n","            d_dom_str, i_dom_str = _fmt_domain_strings(val_dom, num_classes=NUM_CLASSES)\n","            val_row[\"dice_domain\"] = d_dom_str\n","            val_row[\"iou_domain\"]  = i_dom_str\n","\n","        d_cls_str, i_cls_str = _fmt_class_strings(\n","            val_stats[\"dice_per_class\"], val_stats[\"iou_per_class\"],\n","            class_names=globals().get(\"CLASS_NAMES\", None)\n","        )\n","        val_row[\"dice_classes\"] = d_cls_str\n","        val_row[\"iou_classes\"]  = i_cls_str\n","\n","        # ---- Print (same rows that go to XL/CSV) ----\n","        _print_epoch_table(train_row=train_row, val_row=val_row)\n","\n","        # ---- XL/CSV write ----\n","        append_log_row(model_key, train_row)\n","        append_log_row(model_key, val_row)\n","\n","        # Best checkpoint selection: HARD val dice macro\n","        improved = val_hard_dice_macro > best_val_hard_dice_macro + 1e-6\n","\n","        if improved:\n","            best_val_hard_dice_macro = float(val_hard_dice_macro)\n","            best_epoch               = int(epoch)\n","            no_improve_streak        = 0\n","            no_improve_for_lr        = 0\n","\n","            torch.save({\n","                \"epoch\": int(epoch),\n","                \"model_state\": model.state_dict(),\n","                \"optimizer_state\": optimizer.state_dict(),\n","                \"scaler_state\": SCALER.state_dict() if SCALER is not None else None,\n","                \"rng_state\": _get_rng_state_payload(),\n","\n","                \"best_val_hard_dice_macro\": float(best_val_hard_dice_macro),\n","                \"best_epoch\": int(best_epoch),\n","                \"current_lr\": float(current_lr),\n","                \"no_improve_streak\": int(no_improve_streak),\n","                \"no_improve_for_lr\": int(no_improve_for_lr),\n","\n","                \"ce_mode\": ce_mode,\n","                \"abl_fusion\": abl_fus,\n","                \"use_bn\": use_bn,\n","                \"use_gn\": use_gn,\n","                \"base_ch\": base_ch,\n","                \"ds_on\": ds_on,\n","\n","            }, best_ckpt)\n","            print(f\"   ‚úÖ New BEST (valHardDice={val_hard_dice_macro:.4f}) ‚Üí {best_ckpt}\")\n","        else:\n","            no_improve_streak += 1\n","            no_improve_for_lr += 1\n","            print(f\"   ‚ö†Ô∏è  No improvement. Streak={no_improve_streak} (LR-streak={no_improve_for_lr})\")\n","\n","        # Always save LAST checkpoint (with optimizer/scaler)\n","        torch.save({\n","            \"epoch\": int(epoch),\n","            \"model_state\": model.state_dict(),\n","            \"optimizer_state\": optimizer.state_dict(),\n","            \"scaler_state\": SCALER.state_dict() if SCALER is not None else None,\n","            \"rng_state\": _get_rng_state_payload(),\n","\n","            \"best_val_hard_dice_macro\": float(best_val_hard_dice_macro),\n","            \"best_epoch\": int(best_epoch),\n","            \"current_lr\": float(current_lr),\n","            \"no_improve_streak\": int(no_improve_streak),\n","            \"no_improve_for_lr\": int(no_improve_for_lr),\n","\n","            \"ce_mode\": ce_mode,\n","            \"abl_fusion\": abl_fus,\n","            \"use_bn\": use_bn,\n","            \"use_gn\": use_gn,\n","            \"base_ch\": base_ch,\n","            \"ds_on\": ds_on,\n","\n","        }, last_ckpt)\n","        print(f\"   üíæ LAST saved ‚Üí {last_ckpt}\")\n","\n","        # LR decay\n","        if no_improve_for_lr >= LR_PATIENCE_EPOCHS and current_lr > MIN_LR + 1e-12:\n","            new_lr = max(current_lr / LR_DECAY_FACTOR, MIN_LR)\n","            if new_lr < current_lr:\n","                current_lr = float(new_lr)\n","                for g in optimizer.param_groups:\n","                    g[\"lr\"] = current_lr\n","                no_improve_for_lr = 0\n","                print(f\"   üîª LR decayed ‚Üí {current_lr:.6f}\")\n","\n","        # Early stop\n","        if no_improve_streak >= EARLY_STOP_PATIENCE:\n","            print(f\"\\nüõë Early stopping: no improvement for {no_improve_streak} epochs.\")\n","            break\n","\n","    print(\"\\n=== Training finished ===\")\n","    print(f\"   Best val HARD Dice (macro) = {best_val_hard_dice_macro:.4f} @ epoch {best_epoch}\")\n","    print(f\"   BEST ckpt: {best_ckpt}\")\n","    print(f\"   LAST ckpt: {last_ckpt}\")\n","    print(f\"   Logs: {paths['log_csv']} & {paths['log_xlsx']}\")\n","\n","\n","# ---------- Interactive selection ----------\n","print(\"\\n================ MODEL MENU ================\")\n","print(\"  1) UNet\")\n","print(\"  2) UNet3+ (official-style)\")\n","print(\"  3) SS-UNet3+ Lite V1  (EnhSemiFull)\")\n","print(\"  4) SS-UNet3+ Lite V2  (4-Branch)\")\n","print(\"  5) SS-UNet3+ Lite V3  (Baseline)\")\n","print(\"  9) ALL (train all 5 models one by one)\")\n","print(\"===========================================\\n\")\n","\n","choice = input(\"üëâ Choose model (1/2/3/4/5 or 9=ALL): \").strip()\n","\n","choice_map = {\n","    \"1\": \"unet\",\n","    \"2\": \"unet3plus\",\n","    \"3\": \"sslite_v1\",\n","    \"4\": \"sslite_v2\",\n","    \"5\": \"sslite_v3\",\n","    \"9\": \"ALL\",\n","}\n","\n","if choice_map.get(choice, None) == \"ALL\":\n","    selected_models = [\"unet\", \"unet3plus\", \"sslite_v1\", \"sslite_v2\", \"sslite_v3\"]\n","else:\n","    selected_models = [choice_map[choice]]\n","\n","extra_epochs = int(input(\"‚è±  Extra epochs to train (add on top of previous)? (e.g. 10): \").strip())\n","\n","print(\"\\nüìå Will train models:\", selected_models)\n","print(f\"   Extra epochs = {extra_epochs}, initial LR = {INIT_LR}\")\n","print(f\"   AMP enabled?  {USE_AMP}\")\n","\n","for mk in selected_models:\n","    run_training_for_model(mk, extra_epochs=extra_epochs)\n","\n","print(\"\\nüéâ All requested trainings are complete.\")\n"]},{"cell_type":"markdown","metadata":{"id":"Z3_7fIuaUbp1"},"source":["* Downlaod VM folders of Results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQklGfKFUjlf"},"outputs":[],"source":["# ================================\n","# ZIP + Download results folder (uses Cell-1 variables)\n","# ================================\n","import os, shutil\n","from pathlib import Path\n","from google.colab import files\n","\n","# --- Must come from Cell 1 ---\n","assert \"RESULTS_ROOT\" in globals(), \"RESULTS_ROOT not found. Run Cell 1 first.\"\n","assert \"RESULTS_FOLDER_NAME\" in globals(), \"RESULTS_FOLDER_NAME not found. Run Cell 1 first.\"\n","assert \"SAVE_TO_DRIVE\" in globals(), \"SAVE_TO_DRIVE not found. Run Cell 1 first.\"\n","\n","RESULTS_ROOT = Path(RESULTS_ROOT)\n","RESULTS_FOLDER_NAME = str(RESULTS_FOLDER_NAME)\n","\n","# (Option 2 ‡¶π‡¶≤‡ßá SAVE_TO_DRIVE=False ‡¶π‡¶¨‡ßá, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶è‡¶ü‡¶æ ‡¶¶‡ßÅ‡¶á ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá‡¶á ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶¨‡ßá)\n","print(\"SAVE_TO_DRIVE:\", SAVE_TO_DRIVE)\n","print(\"RESULTS_FOLDER_NAME:\", RESULTS_FOLDER_NAME)\n","print(\"RESULTS_ROOT:\", RESULTS_ROOT)\n","\n","assert RESULTS_ROOT.exists() and RESULTS_ROOT.is_dir(), f\"Results folder not found: {RESULTS_ROOT}\"\n","\n","# Zip will be created in /content so Colab can download it easily\n","zip_base = Path(\"/content\") / RESULTS_FOLDER_NAME\n","zip_path = shutil.make_archive(\n","    base_name=str(zip_base),       # /content/<foldername>\n","    format=\"zip\",\n","    root_dir=str(RESULTS_ROOT.parent),\n","    base_dir=str(RESULTS_ROOT.name)\n",")\n","\n","print(\"üì¶ Zipped to:\", zip_path)\n","\n","# Download\n","files.download(zip_path)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}